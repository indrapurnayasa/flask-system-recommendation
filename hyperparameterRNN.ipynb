{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hh/yvtjm3s173l6bkfz3ydwcj9023vg_6/T/ipykernel_50396/4035670448.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  transaksi_df = pd.read_sql_query(\"SELECT * FROM transactions WHERE status_transaction = 'D' and date >= CURRENT_DATE - INTERVAL '90 days'\", conn)\n",
      "/var/folders/hh/yvtjm3s173l6bkfz3ydwcj9023vg_6/T/ipykernel_50396/4035670448.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  promo_df = pd.read_sql_query('SELECT * FROM promo', conn)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "import psycopg2\n",
    "\n",
    "# Replace with your own database connection details\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "# Load transaction data into a Pandas DataFrame\n",
    "transaksi_df = pd.read_sql_query(\"SELECT * FROM transactions WHERE status_transaction = 'D' and date >= CURRENT_DATE - INTERVAL '90 days'\", conn)\n",
    "\n",
    "# Load promo data into a Pandas DataFrame\n",
    "promo_df = pd.read_sql_query('SELECT * FROM promo', conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/testEnv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'embedding_dim': 128, 'lstm_units': 128, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 20}, Mean validation accuracy: 0.0930\n",
      "Params: {'embedding_dim': 192, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.0005, 'batch_size': 64, 'epochs': 50}, Mean validation accuracy: 0.0932\n",
      "Params: {'embedding_dim': 256, 'lstm_units': 512, 'dropout_rate': 0.4, 'learning_rate': 0.0001, 'batch_size': 128, 'epochs': 100}, Mean validation accuracy: 0.0930\n",
      "\n",
      "Best Hyperparameters: {'embedding_dim': 192, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 0.0005, 'batch_size': 64, 'epochs': 50}\n",
      "Best Score: 0.09323467165231705\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Preprocessing Data\n",
    "def preprocess_data(transaksi_df):\n",
    "    le_merchant = LabelEncoder()\n",
    "    le_cif = LabelEncoder()\n",
    "    \n",
    "    transaksi_df['merchant_encoded'] = le_merchant.fit_transform(transaksi_df['merchant_name'])\n",
    "    transaksi_df['cif_encoded'] = le_cif.fit_transform(transaksi_df['cif'])\n",
    "    \n",
    "    return transaksi_df, le_merchant, le_cif\n",
    "\n",
    "# Prepare sequences\n",
    "def prepare_sequences(transaksi_df, sequence_length=10):\n",
    "    transaksi_df = transaksi_df.sort_values(by=['cif', 'date'])\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for cif, group in transaksi_df.groupby('cif_encoded'):\n",
    "        merchant_list = group['merchant_encoded'].tolist()\n",
    "        \n",
    "        for i in range(len(merchant_list) - sequence_length):\n",
    "            sequences.append(merchant_list[i:i + sequence_length])\n",
    "            targets.append(merchant_list[i + sequence_length])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Define the RNN Model\n",
    "def build_model(num_merchants, sequence_length, embedding_dim, lstm_units, dropout_rate, learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(input_dim=num_merchants, output_dim=embedding_dim, input_length=sequence_length),\n",
    "        LSTM(lstm_units, return_sequences=False),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_merchants, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to perform manual grid search\n",
    "def manual_grid_search(X, y, num_merchants, param_grid, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for params in param_grid:\n",
    "        scores = []\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "            \n",
    "            model = build_model(num_merchants=num_merchants,\n",
    "                                sequence_length=X.shape[1],\n",
    "                                embedding_dim=params['embedding_dim'],\n",
    "                                lstm_units=params['lstm_units'],\n",
    "                                dropout_rate=params['dropout_rate'],\n",
    "                                learning_rate=params['learning_rate'])\n",
    "            \n",
    "            history = model.fit(X_train, y_train, \n",
    "                                epochs=params['epochs'], \n",
    "                                batch_size=params['batch_size'],\n",
    "                                validation_data=(X_val, y_val),\n",
    "                                verbose=0)\n",
    "            \n",
    "            val_accuracy = max(history.history['val_accuracy'])\n",
    "            scores.append(val_accuracy)\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"Params: {params}, Mean validation accuracy: {mean_score:.4f}\")\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming transaksi_df is already loaded\n",
    "    transaksi_df, le_merchant, le_cif = preprocess_data(transaksi_df)\n",
    "\n",
    "    # Prepare sequences\n",
    "    sequences, targets = prepare_sequences(transaksi_df)\n",
    "\n",
    "    # Get the number of unique merchants\n",
    "    num_merchants = len(le_merchant.classes_)\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = [\n",
    "        {'embedding_dim': 128, 'lstm_units': 128, 'dropout_rate': 0.2, 'learning_rate': 1e-3, 'batch_size': 32, 'epochs': 20},\n",
    "        {'embedding_dim': 192, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 5e-4, 'batch_size': 64, 'epochs': 50},\n",
    "        {'embedding_dim': 256, 'lstm_units': 512, 'dropout_rate': 0.4, 'learning_rate': 1e-4, 'batch_size': 128, 'epochs': 100}\n",
    "    ]\n",
    "    \n",
    "    # Perform manual grid search\n",
    "    best_params, best_score = manual_grid_search(sequences, targets, num_merchants, param_grid)\n",
    "    \n",
    "    print(\"\\nBest Hyperparameters:\", best_params)\n",
    "    print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Preprocessing Data\n",
    "def preprocess_data(transaksi_df):\n",
    "    le_merchant = LabelEncoder()\n",
    "    le_cif = LabelEncoder()\n",
    "    \n",
    "    transaksi_df['merchant_encoded'] = le_merchant.fit_transform(transaksi_df['merchant_name'])\n",
    "    transaksi_df['cif_encoded'] = le_cif.fit_transform(transaksi_df['cif'])\n",
    "    \n",
    "    return transaksi_df, le_merchant, le_cif\n",
    "\n",
    "# Prepare sequences\n",
    "def prepare_sequences(transaksi_df, sequence_length=10):\n",
    "    transaksi_df = transaksi_df.sort_values(by=['cif', 'date'])\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for cif, group in transaksi_df.groupby('cif_encoded'):\n",
    "        merchant_list = group['merchant_encoded'].tolist()\n",
    "        \n",
    "        for i in range(len(merchant_list) - sequence_length):\n",
    "            sequences.append(merchant_list[i:i + sequence_length])\n",
    "            targets.append(merchant_list[i + sequence_length])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Define the RNN Model\n",
    "def build_model(num_merchants, sequence_length, embedding_dim, lstm_units, dropout_rate, learning_rate, lstm_activation):\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(input_dim=num_merchants, output_dim=embedding_dim, input_length=sequence_length),\n",
    "        LSTM(lstm_units, return_sequences=False, activation=lstm_activation),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_merchants, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to perform manual grid search\n",
    "def manual_grid_search(X, y, num_merchants, param_grid, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for params in param_grid:\n",
    "        scores = []\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "            \n",
    "            model = build_model(num_merchants=num_merchants,\n",
    "                                sequence_length=X.shape[1],\n",
    "                                embedding_dim=params['embedding_dim'],\n",
    "                                lstm_units=params['lstm_units'],\n",
    "                                dropout_rate=params['dropout_rate'],\n",
    "                                learning_rate=params['learning_rate'],\n",
    "                                lstm_activation=params['lstm_activation'])\n",
    "            \n",
    "            history = model.fit(X_train, y_train, \n",
    "                                epochs=params['epochs'], \n",
    "                                batch_size=params['batch_size'],\n",
    "                                validation_data=(X_val, y_val),\n",
    "                                verbose=0)\n",
    "            \n",
    "            val_accuracy = max(history.history['val_accuracy'])\n",
    "            scores.append(val_accuracy)\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"Params: {params}, Mean validation accuracy: {mean_score:.4f}\")\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming transaksi_df is already loaded\n",
    "    transaksi_df, le_merchant, le_cif = preprocess_data(transaksi_df)\n",
    "\n",
    "    # Prepare sequences\n",
    "    sequences, targets = prepare_sequences(transaksi_df)\n",
    "\n",
    "    # Get the number of unique merchants\n",
    "    num_merchants = len(le_merchant.classes_)\n",
    "    \n",
    "    # Define an expanded parameter grid with LSTM activation functions\n",
    "    param_grid = [\n",
    "        {'embedding_dim': 64, 'lstm_units': 64, 'dropout_rate': 0.1, 'learning_rate': 1e-3, 'batch_size': 32, 'epochs': 20, 'lstm_activation': 'tanh'},\n",
    "        {'embedding_dim': 128, 'lstm_units': 128, 'dropout_rate': 0.2, 'learning_rate': 5e-4, 'batch_size': 64, 'epochs': 30, 'lstm_activation': 'relu'},\n",
    "        {'embedding_dim': 192, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 1e-4, 'batch_size': 128, 'epochs': 40, 'lstm_activation': 'tanh'},\n",
    "        {'embedding_dim': 256, 'lstm_units': 512, 'dropout_rate': 0.4, 'learning_rate': 5e-5, 'batch_size': 256, 'epochs': 50, 'lstm_activation': 'relu'},\n",
    "        {'embedding_dim': 384, 'lstm_units': 384, 'dropout_rate': 0.25, 'learning_rate': 7.5e-4, 'batch_size': 96, 'epochs': 35, 'lstm_activation': 'sigmoid'},\n",
    "        {'embedding_dim': 512, 'lstm_units': 768, 'dropout_rate': 0.35, 'learning_rate': 2.5e-4, 'batch_size': 192, 'epochs': 45, 'lstm_activation': 'hard_sigmoid'},\n",
    "    ]\n",
    "    \n",
    "    # Perform manual grid search\n",
    "    best_params, best_score = manual_grid_search(sequences, targets, num_merchants, param_grid)\n",
    "    \n",
    "    print(\"\\nBest Hyperparameters:\", best_params)\n",
    "    print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Preprocessing Data\n",
    "def preprocess_data(transaksi_df):\n",
    "    le_merchant = LabelEncoder()\n",
    "    le_cif = LabelEncoder()\n",
    "    \n",
    "    transaksi_df['merchant_encoded'] = le_merchant.fit_transform(transaksi_df['merchant_name'])\n",
    "    transaksi_df['cif_encoded'] = le_cif.fit_transform(transaksi_df['cif'])\n",
    "    \n",
    "    return transaksi_df, le_merchant, le_cif\n",
    "\n",
    "# Prepare sequences\n",
    "def prepare_sequences(transaksi_df, sequence_length=10):\n",
    "    transaksi_df = transaksi_df.sort_values(by=['cif', 'date'])\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for cif, group in transaksi_df.groupby('cif_encoded'):\n",
    "        merchant_list = group['merchant_encoded'].tolist()\n",
    "        \n",
    "        for i in range(len(merchant_list) - sequence_length):\n",
    "            sequences.append(merchant_list[i:i + sequence_length])\n",
    "            targets.append(merchant_list[i + sequence_length])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Define the RNN Model\n",
    "def build_model(num_merchants, sequence_length, embedding_dim, lstm_units, dropout_rate, learning_rate, lstm_activation):\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(input_dim=num_merchants, output_dim=embedding_dim, input_length=sequence_length),\n",
    "        LSTM(lstm_units, return_sequences=False, activation=lstm_activation),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_merchants, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to perform manual grid search\n",
    "def manual_grid_search(X, y, num_merchants, param_grid, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_params = None\n",
    "    \n",
    "    for params in param_grid:\n",
    "        scores = []\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "            \n",
    "            model = build_model(num_merchants=num_merchants,\n",
    "                                sequence_length=X.shape[1],\n",
    "                                embedding_dim=params['embedding_dim'],\n",
    "                                lstm_units=params['lstm_units'],\n",
    "                                dropout_rate=params['dropout_rate'],\n",
    "                                learning_rate=params['learning_rate'],\n",
    "                                lstm_activation=params['lstm_activation'])\n",
    "            \n",
    "            history = model.fit(X_train, y_train, \n",
    "                                epochs=params['epochs'], \n",
    "                                batch_size=params['batch_size'],\n",
    "                                validation_data=(X_val, y_val),\n",
    "                                verbose=0)\n",
    "            \n",
    "            val_accuracy = max(history.history['val_accuracy'])\n",
    "            scores.append(val_accuracy)\n",
    "        \n",
    "        mean_score = np.mean(scores)\n",
    "        print(f\"Params: {params}, Mean validation accuracy: {mean_score:.4f}\")\n",
    "        \n",
    "        if mean_score > best_score:\n",
    "            best_score = mean_score\n",
    "            best_params = params\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming transaksi_df is already loaded\n",
    "    transaksi_df, le_merchant, le_cif = preprocess_data(transaksi_df)\n",
    "\n",
    "    # Prepare sequences\n",
    "    sequences, targets = prepare_sequences(transaksi_df)\n",
    "\n",
    "    # Get the number of unique merchants\n",
    "    num_merchants = len(le_merchant.classes_)\n",
    "    \n",
    "    # Define an expanded parameter grid\n",
    "    param_grid = [\n",
    "        # Original configurations\n",
    "        {'embedding_dim': 128, 'lstm_units': 128, 'dropout_rate': 0.2, 'learning_rate': 1e-3, 'batch_size': 32, 'epochs': 20, 'lstm_activation': 'tanh'},\n",
    "        {'embedding_dim': 192, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 5e-4, 'batch_size': 64, 'epochs': 50, 'lstm_activation': 'tanh'},\n",
    "        {'embedding_dim': 256, 'lstm_units': 512, 'dropout_rate': 0.4, 'learning_rate': 1e-4, 'batch_size': 128, 'epochs': 100, 'lstm_activation': 'tanh'},\n",
    "        \n",
    "        # Variations on embedding dimensions\n",
    "        {'embedding_dim': 64, 'lstm_units': 128, 'dropout_rate': 0.2, 'learning_rate': 1e-3, 'batch_size': 32, 'epochs': 30, 'lstm_activation': 'relu'},\n",
    "        {'embedding_dim': 320, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 5e-4, 'batch_size': 64, 'epochs': 60, 'lstm_activation': 'tanh'},\n",
    "        \n",
    "        # Variations on LSTM units\n",
    "        {'embedding_dim': 192, 'lstm_units': 64, 'dropout_rate': 0.3, 'learning_rate': 5e-4, 'batch_size': 64, 'epochs': 40, 'lstm_activation': 'relu'},\n",
    "        {'embedding_dim': 256, 'lstm_units': 384, 'dropout_rate': 0.4, 'learning_rate': 1e-4, 'batch_size': 128, 'epochs': 80, 'lstm_activation': 'tanh'},\n",
    "        \n",
    "        # Variations on dropout rate\n",
    "        {'embedding_dim': 128, 'lstm_units': 128, 'dropout_rate': 0.1, 'learning_rate': 1e-3, 'batch_size': 32, 'epochs': 25, 'lstm_activation': 'sigmoid'},\n",
    "        {'embedding_dim': 256, 'lstm_units': 512, 'dropout_rate': 0.5, 'learning_rate': 1e-4, 'batch_size': 128, 'epochs': 90, 'lstm_activation': 'tanh'},\n",
    "        \n",
    "        # Variations on learning rate\n",
    "        {'embedding_dim': 192, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 1e-5, 'batch_size': 64, 'epochs': 70, 'lstm_activation': 'relu'},\n",
    "        {'embedding_dim': 256, 'lstm_units': 512, 'dropout_rate': 0.4, 'learning_rate': 5e-3, 'batch_size': 128, 'epochs': 40, 'lstm_activation': 'tanh'},\n",
    "        \n",
    "        # Variations on batch size\n",
    "        {'embedding_dim': 128, 'lstm_units': 128, 'dropout_rate': 0.2, 'learning_rate': 1e-3, 'batch_size': 16, 'epochs': 30, 'lstm_activation': 'hard_sigmoid'},\n",
    "        {'embedding_dim': 256, 'lstm_units': 512, 'dropout_rate': 0.4, 'learning_rate': 1e-4, 'batch_size': 256, 'epochs': 60, 'lstm_activation': 'tanh'},\n",
    "        \n",
    "        # Variations on epochs\n",
    "        {'embedding_dim': 192, 'lstm_units': 256, 'dropout_rate': 0.3, 'learning_rate': 5e-4, 'batch_size': 64, 'epochs': 30, 'lstm_activation': 'relu'},\n",
    "        {'embedding_dim': 256, 'lstm_units': 512, 'dropout_rate': 0.4, 'learning_rate': 1e-4, 'batch_size': 128, 'epochs': 150, 'lstm_activation': 'tanh'},\n",
    "        \n",
    "        # Some additional combinations\n",
    "        {'embedding_dim': 160, 'lstm_units': 320, 'dropout_rate': 0.35, 'learning_rate': 2.5e-4, 'batch_size': 96, 'epochs': 75, 'lstm_activation': 'tanh'},\n",
    "        {'embedding_dim': 224, 'lstm_units': 448, 'dropout_rate': 0.45, 'learning_rate': 7.5e-5, 'batch_size': 160, 'epochs': 120, 'lstm_activation': 'relu'},\n",
    "    ]\n",
    "    \n",
    "    # Perform manual grid search\n",
    "    best_params, best_score = manual_grid_search(sequences, targets, num_merchants, param_grid)\n",
    "    \n",
    "    print(\"\\nBest Hyperparameters:\", best_params)\n",
    "    print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
