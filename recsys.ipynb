{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import svds\n",
    "import psycopg2\n",
    "\n",
    "# Replace with your own database connection details\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hh/yvtjm3s173l6bkfz3ydwcj9023vg_6/T/ipykernel_76902/1269653763.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  transaksi_df = pd.read_sql_query('SELECT * FROM transactions', conn)\n",
      "/var/folders/hh/yvtjm3s173l6bkfz3ydwcj9023vg_6/T/ipykernel_76902/1269653763.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  promo_df = pd.read_sql_query('SELECT * FROM promo', conn)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>promo_name</th>\n",
       "      <th>category</th>\n",
       "      <th>merchant_name</th>\n",
       "      <th>startdate</th>\n",
       "      <th>enddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dapatkan Potongan Rp100.000 dengan QRIS Wondr ...</td>\n",
       "      <td>6</td>\n",
       "      <td>Grand Lucky</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dapatkan Penawaran Menarik dengan Kartu Debit ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Aljazeerah Signature</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dapatkan Bonus Kuota hingga 10 GB</td>\n",
       "      <td>2</td>\n",
       "      <td>Smartfreen</td>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>2024-09-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dapatkan Bonus Goldie 70% dan Diskon hingga Rp...</td>\n",
       "      <td>5</td>\n",
       "      <td>Funworld</td>\n",
       "      <td>2024-07-05</td>\n",
       "      <td>2024-09-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>wondr by BNI #Jadiinmaumu Tiap Hari Senin</td>\n",
       "      <td>1</td>\n",
       "      <td>Fore</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>2024-10-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                         promo_name  category  \\\n",
       "0  1  Dapatkan Potongan Rp100.000 dengan QRIS Wondr ...         6   \n",
       "1  2  Dapatkan Penawaran Menarik dengan Kartu Debit ...         1   \n",
       "2  3                  Dapatkan Bonus Kuota hingga 10 GB         2   \n",
       "3  4  Dapatkan Bonus Goldie 70% dan Diskon hingga Rp...         5   \n",
       "4  5          wondr by BNI #Jadiinmaumu Tiap Hari Senin         1   \n",
       "\n",
       "          merchant_name   startdate     enddate  \n",
       "0           Grand Lucky        None  2024-12-31  \n",
       "1  Aljazeerah Signature        None  2024-12-31  \n",
       "2            Smartfreen  2024-09-06  2024-09-12  \n",
       "3              Funworld  2024-07-05  2024-09-30  \n",
       "4                  Fore  2024-08-05  2024-10-28  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load transaction data into a Pandas DataFrame\n",
    "transaksi_df = pd.read_sql_query('SELECT * FROM transactions', conn)\n",
    "transaksi_df.head()  # Display the first few rows of the transactions table\n",
    "\n",
    "# Load promo data into a Pandas DataFrame\n",
    "promo_df = pd.read_sql_query('SELECT * FROM promo', conn)\n",
    "promo_df.head()  # Display the first few rows of the promo table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CONTENT BASED FILTERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_promos_content_based(cif, num_recommendations=5):\n",
    "    # Get the user's transaction history\n",
    "    user_transactions = transaksi_df[transaksi_df['cif'] == cif]\n",
    "    # print(\"user transactions\",user_transactions)\n",
    "    \n",
    "    # Initialize a list to store the recommended promotions\n",
    "    recommendations = []\n",
    "    \n",
    "    # Iterate through the user's transactions to recommend promotions based on merchant_name\n",
    "    for _, transaction in user_transactions.iterrows():\n",
    "        user_merchant = transaction['category']\n",
    "        # print(\"user merchant\",user_merchant)\n",
    "        \n",
    "        # Find promotions that match the transaction's merchant_name\n",
    "        merchant_promos = promo_df[promo_df['category'] == user_merchant]\n",
    "        # print(\"merhcant promoooss\",merchant_promos)\n",
    "        \n",
    "        # Add the promo names to the recommendations list\n",
    "        if not merchant_promos.empty:\n",
    "            recommendations.extend(merchant_promos['promo_name'].tolist())\n",
    "        \n",
    "        # If we have enough recommendations, stop the process\n",
    "        if len(recommendations) >= num_recommendations:\n",
    "            break\n",
    "    \n",
    "    # If not enough recommendations, return what we have\n",
    "    return recommendations[:num_recommendations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example recommendation for a specific user\n",
    "cif = '2024253459'  # Replace with an actual account number from your data\n",
    "recommended_promos = recommend_promos_content_based(cif)\n",
    "print(f\"Recommended promotions for account {cif}: {recommended_promos}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended promotions for account 2023188761: [['25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'], ['38', 'Get 40% Discount on All Laser & Hifu Treatments with BNI Credit & Debit Card'], ['39', 'Buy 1 Get 1 Spa & Massage with BNI Credit and Debit Card'], ['43', 'Dapatkan Cicilan 0% hingga 12 bulan dengan Kartu Kredit BNI'], ['3', 'Dapatkan Bonus Kuota hingga 10 GB']]\n",
      "Recommended promotions for account 2023433417: [['25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'], ['38', 'Get 40% Discount on All Laser & Hifu Treatments with BNI Credit & Debit Card'], ['39', 'Buy 1 Get 1 Spa & Massage with BNI Credit and Debit Card'], ['43', 'Dapatkan Cicilan 0% hingga 12 bulan dengan Kartu Kredit BNI'], ['54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot']]\n",
      "Recommended promotions for account 2022734196: [['25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'], ['38', 'Get 40% Discount on All Laser & Hifu Treatments with BNI Credit & Debit Card'], ['39', 'Buy 1 Get 1 Spa & Massage with BNI Credit and Debit Card'], ['43', 'Dapatkan Cicilan 0% hingga 12 bulan dengan Kartu Kredit BNI'], ['54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot']]\n",
      "Recommended promotions for account 2024688533: [['54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot'], ['55', 'Servis Kendaraan di Honda Kencana Kranji bisa Semakin Hemat dengan Cicilan 0% s.d. 12 Bulan'], ['57', 'Diskon 20% hingga Rp15.000 dengan Kartu Kredit BNI'], ['61', 'Diskon hingga 90% untuk Pengguna Baru Grab dengan Kartu Kredit dan Debit BNI'], ['63', 'Dapatkan DIskon hingga Rp460.000 dengan Kartu Debit BNI']]\n",
      "Recommended promotions for account 2022322084: [['7', 'Dapatkan Penawaran Menarik dengan Kartu BNI dan wondr by BNI'], ['7', 'Dapatkan Penawaran Menarik dengan Kartu BNI dan wondr by BNI'], ['3', 'Dapatkan Bonus Kuota hingga 10 GB'], ['56', 'Cashback hingga Rp300.000 dengan Kartu Kredit BNI'], ['58', 'Dapatkan Penawaran Menarik Dengan Kartu Kredit BNI']]\n",
      "Recommended promotions for account 2024504637: [['25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'], ['38', 'Get 40% Discount on All Laser & Hifu Treatments with BNI Credit & Debit Card'], ['39', 'Buy 1 Get 1 Spa & Massage with BNI Credit and Debit Card'], ['43', 'Dapatkan Cicilan 0% hingga 12 bulan dengan Kartu Kredit BNI'], ['7', 'Dapatkan Penawaran Menarik dengan Kartu BNI dan wondr by BNI']]\n",
      "Recommended promotions for account 2024253459: [['2', 'Dapatkan Penawaran Menarik dengan Kartu Debit dan QRIS Wondr by BNI'], ['5', 'wondr by BNI #Jadiinmaumu Tiap Hari Senin'], ['6', 'wondr by BNI #Jadiinmaumu Tiap Hari Senin'], ['8', 'Dapatkan Potongan hingga Rp100.000 dengan QRIS wondr by BNI'], ['9', 'Dapatkan Potongan Langsung Rp150.000 dengan QRIS wondr by BNI']]\n",
      "Recommended promotions for account 2022780533: [['54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot'], ['55', 'Servis Kendaraan di Honda Kencana Kranji bisa Semakin Hemat dengan Cicilan 0% s.d. 12 Bulan'], ['57', 'Diskon 20% hingga Rp15.000 dengan Kartu Kredit BNI'], ['61', 'Diskon hingga 90% untuk Pengguna Baru Grab dengan Kartu Kredit dan Debit BNI'], ['63', 'Dapatkan DIskon hingga Rp460.000 dengan Kartu Debit BNI']]\n",
      "Recommended promotions for account 2024474209: [['54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot'], ['55', 'Servis Kendaraan di Honda Kencana Kranji bisa Semakin Hemat dengan Cicilan 0% s.d. 12 Bulan'], ['57', 'Diskon 20% hingga Rp15.000 dengan Kartu Kredit BNI'], ['61', 'Diskon hingga 90% untuk Pengguna Baru Grab dengan Kartu Kredit dan Debit BNI'], ['63', 'Dapatkan DIskon hingga Rp460.000 dengan Kartu Debit BNI']]\n",
      "Recommended promotions for account 2024391823: [['1', 'Dapatkan Potongan Rp100.000 dengan QRIS Wondr by BNI'], ['23', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'], ['24', 'wondr by BNI #Jadiinmaumu Tiap Hari Kamis'], ['27', 'Diskon hingga Rp150.000 di e-commerce Khusus Nasabah BNI di Hari Pelanggan 2024'], ['28', 'Dapatkan Diskon Rp150.000 dan Cicilan 0% hingga 24 bulan dengan Kartu Kredit BNI JCB']]\n"
     ]
    }
   ],
   "source": [
    "# Updated recommend_promos_content_based function to return promo ids and names\n",
    "def recommend_promos_content_based(cif, num_recommendations=5):\n",
    "    # Get the user's transaction history\n",
    "    user_transactions = transaksi_df[transaksi_df['cif'] == cif]\n",
    "    \n",
    "    # Initialize a list to store the recommended promotions (as tuples of id and name)\n",
    "    recommendations = []\n",
    "    \n",
    "    # Iterate through the user's transactions to recommend promotions based on category\n",
    "    for _, transaction in user_transactions.iterrows():\n",
    "        user_category = transaction['category']\n",
    "        \n",
    "        # Find promotions that match the transaction's category\n",
    "        merchant_promos = promo_df[promo_df['category'] == user_category]\n",
    "        \n",
    "        # Add the promo ids and names to the recommendations list\n",
    "        if not merchant_promos.empty:\n",
    "            recommendations.extend(merchant_promos[['id', 'promo_name']].values.tolist())\n",
    "        \n",
    "        # If we have enough recommendations, stop the process\n",
    "        if len(recommendations) >= num_recommendations:\n",
    "            break\n",
    "    \n",
    "    # Return the recommendations as (id, promo_name) tuples\n",
    "    return recommendations[:num_recommendations]\n",
    "\n",
    "# Get all unique cif values from the transactions DataFrame\n",
    "unique_cifs = transaksi_df['cif'].unique()\n",
    "\n",
    "# Initialize a dictionary to store the recommendations for each cif\n",
    "cif_recommendations = {}\n",
    "\n",
    "# Iterate through each unique cif and generate recommendations\n",
    "for cif in unique_cifs:\n",
    "    # Generate recommendations for the current cif\n",
    "    recommendations = recommend_promos_content_based(cif)\n",
    "    \n",
    "    # Store the recommendations in the dictionary\n",
    "    cif_recommendations[cif] = recommendations\n",
    "    print(f\"Recommended promotions for account {cif}: {recommendations}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATE CONTENT BASED FILTERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recommendations(cif):\n",
    "    # Get user's transaction categories\n",
    "    user_transactions = transaksi_df[transaksi_df['cif'] == cif]\n",
    "    user_categories = user_transactions['category'].unique()\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommended_promos = recommend_promos_content_based(cif)\n",
    "    \n",
    "    # Find the categories of the recommended promotions\n",
    "    recommended_categories = promo_df[promo_df['promo_name'].isin(recommended_promos)]['category'].unique()\n",
    "    \n",
    "    # Calculate precision: proportion of recommended categories that match user's transaction categories\n",
    "    matching_categories = set(recommended_categories).intersection(set(user_categories))\n",
    "    precision = len(matching_categories) / len(recommended_categories) if len(recommended_categories) > 0 else 0\n",
    "    \n",
    "    # Calculate recall: proportion of user's transaction categories that are covered by recommendations\n",
    "    recall = len(matching_categories) / len(user_categories) if len(user_categories) > 0 else 0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "# Evaluate for all users (cifs)\n",
    "cifs = transaksi_df['cif'].unique()\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for cif in cifs:\n",
    "    precision, recall = evaluate_recommendations(cif)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COLLABORATIVE FILTERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create the interaction matrix\n",
    "interaction_matrix = transaksi_df.pivot_table(\n",
    "    index='cif',\n",
    "    columns='category',  # Alternatively, you can use 'category'\n",
    "    values='amount',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "\n",
    "# Convert the interaction matrix to a dense NumPy array\n",
    "interaction_matrix_np = interaction_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Ensure k is smaller than the smallest dimension of the matrix\n",
    "k = min(50, interaction_matrix_np.shape[0] - 1, interaction_matrix_np.shape[1] - 1)\n",
    "\n",
    "# Perform SVD with the adjusted k\n",
    "U, sigma, Vt = svds(interaction_matrix_np, k=k)\n",
    "\n",
    "# Convert sigma into a diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the predicted interaction matrix\n",
    "predicted_interactions = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "# Convert the result back into a DataFrame for easy interpretation\n",
    "predicted_interactions_df = pd.DataFrame(predicted_interactions, \n",
    "                                         index=interaction_matrix.index,  # Users (cif)\n",
    "                                         columns=interaction_matrix.columns)  # Merchants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_merchants(user_id, num_recommendations=5):\n",
    "    # Get the user's predicted interaction scores for all merchants\n",
    "    user_predictions = predicted_interactions_df.loc[user_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Recommend the top merchants with the highest predicted scores\n",
    "    top_merchants = user_predictions.head(num_recommendations)\n",
    "    \n",
    "    return top_merchants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_promos(user_id, num_recommendations=5):\n",
    "    # Get top merchant recommendations for the user\n",
    "    top_merchants = recommend_merchants(user_id, num_recommendations)\n",
    "\n",
    "    # Filter the promo table for these merchants\n",
    "    recommended_promos = promo_df[promo_df['category'].isin(top_merchants.index)]\n",
    "    \n",
    "    # If no direct match on merchant, fallback to category-based promotions\n",
    "    if recommended_promos.empty:\n",
    "        # Get the user's top transaction categories\n",
    "        user_categories = transaksi_df[transaksi_df['cif'] == user_id]['category'].unique()\n",
    "        recommended_promos = promo_df[promo_df['category'].isin(user_categories)]\n",
    "    \n",
    "    return recommended_promos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                         promo_name  category  \\\n",
      "2    3                  Dapatkan Bonus Kuota hingga 10 GB         2   \n",
      "52  54   Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot         3   \n",
      "53  55  Servis Kendaraan di Honda Kencana Kranji bisa ...         3   \n",
      "54  56  Cashback hingga Rp300.000 dengan Kartu Kredit BNI         2   \n",
      "55  57  Diskon 20% hingga Rp15.000 dengan Kartu Kredit...         3   \n",
      "56  58  Dapatkan Penawaran Menarik Dengan Kartu Kredit...         2   \n",
      "57  59   Cashback Rp75.000 dengan Daftar BNI Bill Payment         2   \n",
      "58  60        Dapatkan Diskon 15% dengan Kartu Kredit BNI         2   \n",
      "59  61  Diskon hingga 90% untuk Pengguna Baru Grab den...         3   \n",
      "61  63  Dapatkan DIskon hingga Rp460.000 dengan Kartu ...         3   \n",
      "62  64  Dapatkan Cicilan 0% hingga 12 Bulan dengan Kar...         3   \n",
      "63  65               Dapatkan DIskon 10% dengan Kartu BNI         3   \n",
      "64  66  Dapatkan Diskon 7% dengan Kartu Kredit, Debit ...         3   \n",
      "\n",
      "           merchant_name   startdate     enddate  \n",
      "2             Smartfreen  2024-09-06  2024-09-12  \n",
      "52      Honda Daan Mogot        None  2024-10-31  \n",
      "53  Honda Kencana Kranji        None  2024-10-31  \n",
      "54              Icon Net        None  2024-09-30  \n",
      "55             Blue Bird  2024-03-01  2024-12-31  \n",
      "56           First Media        None  2024-12-31  \n",
      "57       Promo Utilities        None  2024-12-31  \n",
      "58             Java Mifi        None  2024-12-31  \n",
      "59                  Grab  2024-02-01  2024-12-31  \n",
      "61              Citilink  2024-07-31  2024-09-30  \n",
      "62              Air Asia        None  2024-12-31  \n",
      "63              Emirates        None  2024-12-31  \n",
      "64               Eva AIr  2024-07-01  2025-06-30  \n"
     ]
    }
   ],
   "source": [
    "# Recommend promotions for a user with CIF '123456'\n",
    "user_id = '2024391823'\n",
    "recommendations = recommend_promos(user_id=user_id, num_recommendations=5)\n",
    "\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIF: 2023188761 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "1    2  Dapatkan Penawaran Menarik dengan Kartu Debit ...\n",
      "3    4  Dapatkan Bonus Goldie 70% dan Diskon hingga Rp...\n",
      "4    5          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "5    6          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "6    7  Dapatkan Penawaran Menarik dengan Kartu BNI da...\n",
      "7    8  Dapatkan Potongan hingga Rp100.000 dengan QRIS...\n",
      "8    9  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "9   10  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "10  11  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "11  12  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "\n",
      "CIF: 2023433417 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "0    1  Dapatkan Potongan Rp100.000 dengan QRIS Wondr ...\n",
      "1    2  Dapatkan Penawaran Menarik dengan Kartu Debit ...\n",
      "2    3                  Dapatkan Bonus Kuota hingga 10 GB\n",
      "4    5          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "5    6          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "6    7  Dapatkan Penawaran Menarik dengan Kartu BNI da...\n",
      "7    8  Dapatkan Potongan hingga Rp100.000 dengan QRIS...\n",
      "8    9  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "9   10  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "10  11  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "\n",
      "CIF: 2022734196 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "0    1  Dapatkan Potongan Rp100.000 dengan QRIS Wondr ...\n",
      "1    2  Dapatkan Penawaran Menarik dengan Kartu Debit ...\n",
      "2    3                  Dapatkan Bonus Kuota hingga 10 GB\n",
      "4    5          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "5    6          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "6    7  Dapatkan Penawaran Menarik dengan Kartu BNI da...\n",
      "7    8  Dapatkan Potongan hingga Rp100.000 dengan QRIS...\n",
      "8    9  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "9   10  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "10  11  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "\n",
      "CIF: 2024688533 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "1    2  Dapatkan Penawaran Menarik dengan Kartu Debit ...\n",
      "2    3                  Dapatkan Bonus Kuota hingga 10 GB\n",
      "3    4  Dapatkan Bonus Goldie 70% dan Diskon hingga Rp...\n",
      "4    5          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "5    6          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "6    7  Dapatkan Penawaran Menarik dengan Kartu BNI da...\n",
      "7    8  Dapatkan Potongan hingga Rp100.000 dengan QRIS...\n",
      "8    9  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "9   10  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "10  11  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "\n",
      "CIF: 2022322084 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "1    2  Dapatkan Penawaran Menarik dengan Kartu Debit ...\n",
      "2    3                  Dapatkan Bonus Kuota hingga 10 GB\n",
      "3    4  Dapatkan Bonus Goldie 70% dan Diskon hingga Rp...\n",
      "4    5          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "5    6          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "7    8  Dapatkan Potongan hingga Rp100.000 dengan QRIS...\n",
      "8    9  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "9   10  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "10  11  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "11  12  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "\n",
      "CIF: 2024504637 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "0    1  Dapatkan Potongan Rp100.000 dengan QRIS Wondr ...\n",
      "2    3                  Dapatkan Bonus Kuota hingga 10 GB\n",
      "3    4  Dapatkan Bonus Goldie 70% dan Diskon hingga Rp...\n",
      "6    7  Dapatkan Penawaran Menarik dengan Kartu BNI da...\n",
      "14  15  Dapatkan Diskon Rp30.000 dengan QRIS wondr by ...\n",
      "15  16  Dapatkan Diskon Rp30.000 dengan QRIS wondr by ...\n",
      "16  17  Dapatkan Diskon Rp30.000 dengan QRIS wondr by ...\n",
      "22  23         wondr by BNI #Jadiinmaumu Tiap Hari Minggu\n",
      "23  24          wondr by BNI #Jadiinmaumu Tiap Hari Kamis\n",
      "24  25         wondr by BNI #Jadiinmaumu Tiap Hari Minggu\n",
      "\n",
      "CIF: 2024253459 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "1    2  Dapatkan Penawaran Menarik dengan Kartu Debit ...\n",
      "3    4  Dapatkan Bonus Goldie 70% dan Diskon hingga Rp...\n",
      "4    5          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "5    6          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "6    7  Dapatkan Penawaran Menarik dengan Kartu BNI da...\n",
      "7    8  Dapatkan Potongan hingga Rp100.000 dengan QRIS...\n",
      "8    9  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "9   10  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "10  11  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "11  12  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "\n",
      "CIF: 2022780533 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "1    2  Dapatkan Penawaran Menarik dengan Kartu Debit ...\n",
      "2    3                  Dapatkan Bonus Kuota hingga 10 GB\n",
      "4    5          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "5    6          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "6    7  Dapatkan Penawaran Menarik dengan Kartu BNI da...\n",
      "7    8  Dapatkan Potongan hingga Rp100.000 dengan QRIS...\n",
      "8    9  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "9   10  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "10  11  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "11  12  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "\n",
      "CIF: 2024474209 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "0    1  Dapatkan Potongan Rp100.000 dengan QRIS Wondr ...\n",
      "3    4  Dapatkan Bonus Goldie 70% dan Diskon hingga Rp...\n",
      "6    7  Dapatkan Penawaran Menarik dengan Kartu BNI da...\n",
      "14  15  Dapatkan Diskon Rp30.000 dengan QRIS wondr by ...\n",
      "15  16  Dapatkan Diskon Rp30.000 dengan QRIS wondr by ...\n",
      "16  17  Dapatkan Diskon Rp30.000 dengan QRIS wondr by ...\n",
      "17  18          Cahback Rp50.000 dengan QRIS wondr by BNI\n",
      "22  23         wondr by BNI #Jadiinmaumu Tiap Hari Minggu\n",
      "23  24          wondr by BNI #Jadiinmaumu Tiap Hari Kamis\n",
      "24  25         wondr by BNI #Jadiinmaumu Tiap Hari Minggu\n",
      "\n",
      "CIF: 2024391823 - Recommended Promotions:\n",
      "    id                                         promo_name\n",
      "0    1  Dapatkan Potongan Rp100.000 dengan QRIS Wondr ...\n",
      "1    2  Dapatkan Penawaran Menarik dengan Kartu Debit ...\n",
      "2    3                  Dapatkan Bonus Kuota hingga 10 GB\n",
      "4    5          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "5    6          wondr by BNI #Jadiinmaumu Tiap Hari Senin\n",
      "7    8  Dapatkan Potongan hingga Rp100.000 dengan QRIS...\n",
      "8    9  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "9   10  Dapatkan Potongan Langsung Rp150.000 dengan QR...\n",
      "10  11  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "11  12  Dapatkan Potongan Langsung Rp100.000 dengan QR...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Create the interaction matrix (pivot table) using 'category' and 'amount'\n",
    "interaction_matrix = transaksi_df.pivot_table(\n",
    "    index='cif',\n",
    "    columns='category',  \n",
    "    values='amount',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Step 2: Convert the interaction matrix to a NumPy array\n",
    "interaction_matrix_np = interaction_matrix.values\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Step 3: Set k as smaller than the smallest matrix dimension and perform SVD\n",
    "k = min(50, interaction_matrix_np.shape[0] - 1, interaction_matrix_np.shape[1] - 1)\n",
    "\n",
    "U, sigma, Vt = svds(interaction_matrix_np, k=k)\n",
    "\n",
    "# Convert sigma into a diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Reconstruct the predicted interaction matrix\n",
    "predicted_interactions = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "# Step 4: Convert the predicted interactions back into a DataFrame\n",
    "predicted_interactions_df = pd.DataFrame(predicted_interactions, \n",
    "                                         index=interaction_matrix.index,  # Users (cif)\n",
    "                                         columns=interaction_matrix.columns)  # Categories\n",
    "\n",
    "# Step 5: Define the recommendation functions\n",
    "\n",
    "def recommend_merchants(user_id, num_recommendations=5):\n",
    "    # Get the user's predicted interaction scores for all categories\n",
    "    user_predictions = predicted_interactions_df.loc[user_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Recommend the top categories with the highest predicted scores\n",
    "    top_categories = user_predictions.head(num_recommendations)\n",
    "    \n",
    "    return top_categories\n",
    "\n",
    "def recommend_promo_cf(user_id, num_recommendations=10):\n",
    "    # Get top category recommendations for the user\n",
    "    top_categories = recommend_merchants(user_id, num_recommendations)\n",
    "\n",
    "    # Filter the promo table for these categories\n",
    "    recommended_promos = promo_df[promo_df['category'].isin(top_categories.index)]\n",
    "    \n",
    "    # If no direct match on category, fallback to categories from user transactions\n",
    "    if recommended_promos.empty:\n",
    "        # Get the user's top transaction categories\n",
    "        user_categories = transaksi_df[transaksi_df['cif'] == user_id]['category'].unique()\n",
    "        recommended_promos = promo_df[promo_df['category'].isin(user_categories)]\n",
    "    \n",
    "    return recommended_promos[['id', 'promo_name']].head(num_recommendations)\n",
    "\n",
    "# Step 6: Generate recommendations for all unique CIFs\n",
    "\n",
    "# Get all unique cif values from the transactions DataFrame\n",
    "unique_cifs = transaksi_df['cif'].unique()\n",
    "\n",
    "# Initialize a dictionary to store the recommendations for each cif\n",
    "cif_recommendations = {}\n",
    "\n",
    "# Iterate through each unique cif and generate recommendations\n",
    "for cif in unique_cifs:\n",
    "    # Generate recommendations for the current cif\n",
    "    recommendations = recommend_promo_cf(cif)\n",
    "    \n",
    "    # Store the recommendations in the dictionary\n",
    "    cif_recommendations[cif] = recommendations\n",
    "\n",
    "# Display the recommendations for each cif\n",
    "for cif, recommendations in cif_recommendations.items():\n",
    "    print(f\"CIF: {cif} - Recommended Promotions:\\n{recommendations}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION COLLABORATIVE FILTERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming transaksi_df and promo_df are already loaded as DataFrames\n",
    "\n",
    "def recommend_merchants(user_id, num_recommendations=5):\n",
    "    # Get the user's predicted interaction scores for all merchants (or categories)\n",
    "    user_predictions = predicted_interactions_df.loc[user_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Recommend the top merchants/categories with the highest predicted scores\n",
    "    top_merchants = user_predictions.head(num_recommendations)\n",
    "    \n",
    "    return top_merchants\n",
    "\n",
    "def recommend_promo_hybrid(user_id, num_recommendations=5):\n",
    "    # Get top merchant/category recommendations for the user\n",
    "    top_merchants = recommend_merchants(user_id, num_recommendations)\n",
    "\n",
    "    # Filter the promo table for these merchants/categories\n",
    "    recommended_promos = promo_df[promo_df['category'].isin(top_merchants.index)]\n",
    "    \n",
    "    # If no direct match on merchant, fallback to category-based promotions\n",
    "    if recommended_promos.empty:\n",
    "        # Get the user's top transaction categories\n",
    "        user_categories = transaksi_df[transaksi_df['cif'] == user_id]['category'].unique()\n",
    "        recommended_promos = promo_df[promo_df['category'].isin(user_categories)]\n",
    "    \n",
    "    return recommended_promos\n",
    "\n",
    "def evaluate_recommendations_svd(cif, num_recommendations=5):\n",
    "    # Get user's transaction categories\n",
    "    user_transactions = transaksi_df[transaksi_df['cif'] == cif]\n",
    "    user_categories = user_transactions['category'].unique()\n",
    "    \n",
    "    # Get recommendations\n",
    "    recommended_promos = recommend_promos(cif, num_recommendations)\n",
    "    \n",
    "    # Find the categories of the recommended promotions\n",
    "    recommended_categories = recommended_promos['category'].unique()\n",
    "    \n",
    "    # Calculate precision: proportion of recommended categories that match user's transaction categories\n",
    "    matching_categories = set(recommended_categories).intersection(set(user_categories))\n",
    "    precision = len(matching_categories) / len(recommended_categories) if len(recommended_categories) > 0 else 0\n",
    "    \n",
    "    # Calculate recall: proportion of user's transaction categories that are covered by recommendations\n",
    "    recall = len(matching_categories) / len(user_categories) if len(user_categories) > 0 else 0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "# Evaluate for all users (cifs)\n",
    "cifs = transaksi_df['cif'].unique()\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for cif in cifs:\n",
    "    precision, recall = evaluate_recommendations_svd(cif, num_recommendations=5)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "avg_precision = sum(precision_scores) / len(precision_scores)\n",
    "avg_recall = sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Assuming transaksi_df and promo_df are already loaded\n",
    "\n",
    "# Create the interaction matrix\n",
    "interaction_matrix = transaksi_df.pivot_table(\n",
    "    index='cif',\n",
    "    columns='category',  # Alternatively, you can use 'category'\n",
    "    values='amount',\n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Convert the interaction matrix to a dense NumPy array\n",
    "interaction_matrix_np = interaction_matrix.values\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Ensure k is smaller than the smallest dimension of the matrix\n",
    "k = min(50, interaction_matrix_np.shape[0] - 1, interaction_matrix_np.shape[1] - 1)\n",
    "\n",
    "# Perform SVD with the adjusted k\n",
    "U, sigma, Vt = svds(interaction_matrix_np, k=k)\n",
    "\n",
    "# Convert sigma into a diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Reconstruct the predicted interaction matrix\n",
    "predicted_interactions = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "# Convert the result back into a DataFrame for easy interpretation\n",
    "predicted_interactions_df = pd.DataFrame(predicted_interactions, \n",
    "                                         index=interaction_matrix.index,  # Users (cif)\n",
    "                                         columns=interaction_matrix.columns)  # Categories\n",
    "\n",
    "def recommend_merchants(user_id, num_recommendations=5):\n",
    "    # Get the user's predicted interaction scores for all merchants\n",
    "    user_predictions = predicted_interactions_df.loc[user_id].sort_values(ascending=False)\n",
    "    \n",
    "    # Recommend the top merchants with the highest predicted scores\n",
    "    top_merchants = user_predictions.head(num_recommendations)\n",
    "    \n",
    "    return top_merchants\n",
    "\n",
    "def recommend_promos(user_id, num_recommendations=5):\n",
    "    # Get top merchant recommendations for the user\n",
    "    top_merchants = recommend_merchants(user_id, num_recommendations)\n",
    "\n",
    "    # Filter the promo table for these merchants\n",
    "    recommended_promos = promo_df[promo_df['category'].isin(top_merchants.index)]\n",
    "    \n",
    "    # If no direct match on merchant, fallback to category-based promotions\n",
    "    if recommended_promos.empty:\n",
    "        # Get the user's top transaction categories\n",
    "        user_categories = transaksi_df[transaksi_df['cif'] == user_id]['category'].unique()\n",
    "        recommended_promos = promo_df[promo_df['category'].isin(user_categories)]\n",
    "    \n",
    "    return recommended_promos\n",
    "\n",
    "# F1 Score Evaluation\n",
    "def evaluate_f1_score(cif, num_recommendations=5):\n",
    "    precision, recall = evaluate_recommendations_svd(cif, num_recommendations)\n",
    "    \n",
    "    if precision + recall == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "# MAP@K Evaluation\n",
    "def average_precision_at_k(recommended_categories, user_categories, k):\n",
    "    num_hits = 0.0\n",
    "    score = 0.0\n",
    "    \n",
    "    for i, category in enumerate(recommended_categories[:k]):\n",
    "        if category in user_categories:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    \n",
    "    return score / min(len(user_categories), k)\n",
    "\n",
    "def evaluate_map_k(cif, k=5):\n",
    "    user_transactions = transaksi_df[transaksi_df['cif'] == cif]\n",
    "    user_categories = user_transactions['category'].unique()\n",
    "    \n",
    "    recommended_promos = recommend_promos(cif, k)\n",
    "    recommended_categories = recommended_promos['category'].tolist()\n",
    "    \n",
    "    return average_precision_at_k(recommended_categories, user_categories, k)\n",
    "\n",
    "# Coverage Evaluation\n",
    "def evaluate_coverage(num_recommendations=5):\n",
    "    all_categories = set(promo_df['category'].unique())\n",
    "    recommended_categories = set()\n",
    "    \n",
    "    for cif in transaksi_df['cif'].unique():\n",
    "        recommended_promos = recommend_promos(cif, num_recommendations)\n",
    "        recommended_categories.update(recommended_promos['category'].unique())\n",
    "    \n",
    "    coverage = len(recommended_categories) / len(all_categories)\n",
    "    return coverage\n",
    "\n",
    "# Diversity Evaluation\n",
    "def evaluate_diversity(cif, num_recommendations=5):\n",
    "    recommended_promos = recommend_promos(cif, num_recommendations)\n",
    "    \n",
    "    if len(recommended_promos) <= 1:\n",
    "        return 0  # No diversity if only one or no promo is recommended\n",
    "    \n",
    "    promo_categories = recommended_promos['category'].tolist()\n",
    "    \n",
    "    # Calculate diversity as the average pairwise cosine similarity\n",
    "    diversity_score = 1 - np.mean([cosine_similarity([promo_categories[i]], [promo_categories[j]])[0][0]\n",
    "                                   for i in range(len(promo_categories))\n",
    "                                   for j in range(i + 1, len(promo_categories))])\n",
    "    \n",
    "    return diversity_score\n",
    "\n",
    "# Diversity Evaluation\n",
    "def evaluate_diversity(cif, num_recommendations=5):\n",
    "    recommended_promos = recommend_promos(cif, num_recommendations)\n",
    "    \n",
    "    if len(recommended_promos) <= 1:\n",
    "        return 0  # No diversity if only one or no promo is recommended\n",
    "    \n",
    "    promo_categories = recommended_promos['category'].tolist()\n",
    "    \n",
    "    # Calculate diversity as the average pairwise cosine similarity\n",
    "    diversity_score = 1 - np.mean([\n",
    "        cosine_similarity(\n",
    "            np.array([promo_categories[i]]).reshape(1, -1), \n",
    "            np.array([promo_categories[j]]).reshape(1, -1)\n",
    "        )[0][0]\n",
    "        for i in range(len(promo_categories))\n",
    "        for j in range(i + 1, len(promo_categories))\n",
    "    ])\n",
    "    \n",
    "    return diversity_score\n",
    "\n",
    "\n",
    "# Calculating evaluation metrics across all users\n",
    "cifs = transaksi_df['cif'].unique()\n",
    "\n",
    "# F1 Score\n",
    "f1_scores = [evaluate_f1_score(cif, 5) for cif in cifs]\n",
    "avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "print(f\"Average F1 Score: {avg_f1_score:.4f}\")\n",
    "\n",
    "# MAP@K\n",
    "map_k_scores = [evaluate_map_k(cif, 5) for cif in cifs]\n",
    "avg_map_k = sum(map_k_scores) / len(map_k_scores)\n",
    "print(f\"Average MAP@K: {avg_map_k:.4f}\")\n",
    "\n",
    "# Coverage\n",
    "coverage_score = evaluate_coverage(5)\n",
    "print(f\"Coverage: {coverage_score:.4f}\")\n",
    "\n",
    "# Diversity\n",
    "diversity_scores = [evaluate_diversity(cif, 5) for cif in cifs]\n",
    "avg_diversity = sum(diversity_scores) / len(diversity_scores)\n",
    "print(f\"Average Diversity: {avg_diversity:.4f}\")\n",
    "\n",
    "# Novelty\n",
    "novelty_scores = [evaluate_novelty(cif, 5) for cif in cifs]\n",
    "avg_novelty = sum(novelty_scores) / len(novelty_scores)\n",
    "print(f\"Average Novelty: {avg_novelty:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYBRID FILTERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table (user-item interaction matrix)\n",
    "user_item_matrix = transaksi_df.pivot_table(index='cif', columns='category', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Save the columns (merchant names) and index (account IDs) for later use\n",
    "merchants = user_item_matrix.columns\n",
    "accounts = user_item_matrix.index\n",
    "\n",
    "# Convert the interaction matrix to a dense NumPy array with a floating-point data type\n",
    "user_item_matrix = user_item_matrix.to_numpy(dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the shape of the interaction matrix\n",
    "num_accounts, num_merchants = user_item_matrix.shape\n",
    "\n",
    "# Set the value of k based on the shape of the matrix (k must be less than min(num_accounts, num_merchants))\n",
    "k = min(num_accounts, num_merchants) - 1\n",
    "\n",
    "# Apply SVD to the user-item matrix\n",
    "U, sigma, Vt = svds(user_item_matrix, k=k)\n",
    "\n",
    "# Convert sigma to a diagonal matrix\n",
    "sigma = np.diag(sigma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the approximate interaction matrix\n",
    "predicted_matrix = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "# Convert the predicted matrix back to a DataFrame for easier interpretation\n",
    "predicted_df = pd.DataFrame(predicted_matrix, index=accounts, columns=merchants)\n",
    "\n",
    "# Display the predicted matrix\n",
    "print(\"Predicted User-Item Interaction Matrix:\")\n",
    "predicted_df.head()  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_promos(cif, num_recommendations=5):\n",
    "    # Get the predicted scores for the given account\n",
    "    user_predictions = predicted_df.loc[cif]\n",
    "    \n",
    "    # Sort merchants by predicted score\n",
    "    sorted_merchants = user_predictions.sort_values(ascending=False)\n",
    "    \n",
    "    # User's transaction data\n",
    "    user_transactions = transaksi_df[transaksi_df['cif'] == cif]\n",
    "    \n",
    "    # Recommend promos that match top merchants or by category\n",
    "    recommendations = []\n",
    "    for merchant in sorted_merchants.index:\n",
    "        # Check if there are promos available for this merchant\n",
    "        merchant_promos = promo_df[promo_df['merchant_name'] == merchant]\n",
    "        if not merchant_promos.empty:\n",
    "            recommendations.extend(merchant_promos[['id', 'promo_name', 'category', 'merchant_name']].values.tolist())\n",
    "        else:\n",
    "            # Check if the user has transactions with this merchant\n",
    "            user_merchant_transactions = user_transactions[user_transactions['merchant_name'] == merchant]\n",
    "            if not user_merchant_transactions.empty:\n",
    "                user_category = user_merchant_transactions['category'].values[0]\n",
    "                # Check if there are promos available for the same category\n",
    "                category_promos = promo_df[promo_df['category'] == user_category]\n",
    "                if not category_promos.empty:\n",
    "                    recommendations.extend(category_promos[['id', 'promo_name', 'category', 'merchant_name']].values.tolist())\n",
    "        \n",
    "        # If we've gathered enough recommendations, break the loop\n",
    "        if len(recommendations) >= num_recommendations:\n",
    "            break\n",
    "    \n",
    "    # If we still don't have enough recommendations, fill with category-based recommendations\n",
    "    if len(recommendations) < num_recommendations:\n",
    "        for _, transaction in user_transactions.iterrows():\n",
    "            category_promos = promo_df[promo_df['category'] == transaction['category']]\n",
    "            if not category_promos.empty:\n",
    "                recommendations.extend(category_promos['promo_name'].tolist())\n",
    "            if len(recommendations) >= num_recommendations:\n",
    "                break\n",
    "    \n",
    "    # Return only the top recommendations\n",
    "    return recommendations[:num_recommendations]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example recommendation for a specific user\n",
    "cif = '2024253459'  # Replace with an actual account number from your data\n",
    "recommended_promos = recommend_promos(cif)\n",
    "print(f\"Recommended promotions for account {cif}: {recommended_promos}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIF: 2023188761 - Recommended Promo IDs: [('25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'), ('38', 'Get 40% Discount on All Laser & Hifu Treatments with BNI Credit & Debit Card'), ('39', 'Buy 1 Get 1 Spa & Massage with BNI Credit and Debit Card'), ('43', 'Dapatkan Cicilan 0% hingga 12 bulan dengan Kartu Kredit BNI'), ('3', 'Dapatkan Bonus Kuota hingga 10 GB'), ('56', 'Cashback hingga Rp300.000 dengan Kartu Kredit BNI'), ('58', 'Dapatkan Penawaran Menarik Dengan Kartu Kredit BNI'), ('59', 'Cashback Rp75.000 dengan Daftar BNI Bill Payment'), ('60', 'Dapatkan Diskon 15% dengan Kartu Kredit BNI'), ('25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu')]\n",
      "\n",
      "CIF: 2023433417 - Recommended Promo IDs: [('25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'), ('38', 'Get 40% Discount on All Laser & Hifu Treatments with BNI Credit & Debit Card'), ('39', 'Buy 1 Get 1 Spa & Massage with BNI Credit and Debit Card'), ('43', 'Dapatkan Cicilan 0% hingga 12 bulan dengan Kartu Kredit BNI'), ('54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot'), ('55', 'Servis Kendaraan di Honda Kencana Kranji bisa Semakin Hemat dengan Cicilan 0% s.d. 12 Bulan'), ('57', 'Diskon 20% hingga Rp15.000 dengan Kartu Kredit BNI'), ('61', 'Diskon hingga 90% untuk Pengguna Baru Grab dengan Kartu Kredit dan Debit BNI'), ('63', 'Dapatkan DIskon hingga Rp460.000 dengan Kartu Debit BNI'), ('64', 'Dapatkan Cicilan 0% hingga 12 Bulan dengan Kartu Kredit BNI')]\n",
      "\n",
      "CIF: 2022734196 - Recommended Promo IDs: [('25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'), ('38', 'Get 40% Discount on All Laser & Hifu Treatments with BNI Credit & Debit Card'), ('39', 'Buy 1 Get 1 Spa & Massage with BNI Credit and Debit Card'), ('43', 'Dapatkan Cicilan 0% hingga 12 bulan dengan Kartu Kredit BNI'), ('54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot'), ('55', 'Servis Kendaraan di Honda Kencana Kranji bisa Semakin Hemat dengan Cicilan 0% s.d. 12 Bulan'), ('57', 'Diskon 20% hingga Rp15.000 dengan Kartu Kredit BNI'), ('61', 'Diskon hingga 90% untuk Pengguna Baru Grab dengan Kartu Kredit dan Debit BNI'), ('63', 'Dapatkan DIskon hingga Rp460.000 dengan Kartu Debit BNI'), ('64', 'Dapatkan Cicilan 0% hingga 12 Bulan dengan Kartu Kredit BNI')]\n",
      "\n",
      "CIF: 2024688533 - Recommended Promo IDs: [('54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot'), ('55', 'Servis Kendaraan di Honda Kencana Kranji bisa Semakin Hemat dengan Cicilan 0% s.d. 12 Bulan'), ('57', 'Diskon 20% hingga Rp15.000 dengan Kartu Kredit BNI'), ('61', 'Diskon hingga 90% untuk Pengguna Baru Grab dengan Kartu Kredit dan Debit BNI'), ('63', 'Dapatkan DIskon hingga Rp460.000 dengan Kartu Debit BNI'), ('64', 'Dapatkan Cicilan 0% hingga 12 Bulan dengan Kartu Kredit BNI'), ('65', 'Dapatkan DIskon 10% dengan Kartu BNI'), ('66', 'Dapatkan Diskon 7% dengan Kartu Kredit, Debit dan Debit BNI Emerald'), ('4', 'Dapatkan Bonus Goldie 70% dan Diskon hingga Rp50.000 dengan QRIS Wondr by BNI, Kartu Debit & Kredit BNI'), ('15', 'Dapatkan Diskon Rp30.000 dengan QRIS wondr by BNI, Kartu Debit & Kredit BNI')]\n",
      "\n",
      "CIF: 2022322084 - Recommended Promo IDs: [('7', 'Dapatkan Penawaran Menarik dengan Kartu BNI dan wondr by BNI'), ('7', 'Dapatkan Penawaran Menarik dengan Kartu BNI dan wondr by BNI'), ('3', 'Dapatkan Bonus Kuota hingga 10 GB'), ('56', 'Cashback hingga Rp300.000 dengan Kartu Kredit BNI'), ('58', 'Dapatkan Penawaran Menarik Dengan Kartu Kredit BNI'), ('59', 'Cashback Rp75.000 dengan Daftar BNI Bill Payment'), ('60', 'Dapatkan Diskon 15% dengan Kartu Kredit BNI'), ('2', 'Dapatkan Penawaran Menarik dengan Kartu Debit dan QRIS Wondr by BNI'), ('5', 'wondr by BNI #Jadiinmaumu Tiap Hari Senin'), ('6', 'wondr by BNI #Jadiinmaumu Tiap Hari Senin')]\n",
      "\n",
      "CIF: 2024504637 - Recommended Promo IDs: [('25', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'), ('38', 'Get 40% Discount on All Laser & Hifu Treatments with BNI Credit & Debit Card'), ('39', 'Buy 1 Get 1 Spa & Massage with BNI Credit and Debit Card'), ('43', 'Dapatkan Cicilan 0% hingga 12 bulan dengan Kartu Kredit BNI'), ('7', 'Dapatkan Penawaran Menarik dengan Kartu BNI dan wondr by BNI'), ('49', 'CIcilan 0% tenor sampai dengan 12 bulan dengan Kartu Kredit BNI'), ('50', 'Dapatkan Cicilan 0% dengan Kartu Kredit BNI'), ('51', 'Cicilan 0% hingga 12 bulan untuk Bayar Biaya Pendidikan dengan Kartu Kredit BNI'), ('52', 'Dapatkan Cicilan 0% s.d. 6 Bulan untuk Pembayaran Biaya Pendidikan dengan Kartu Kredit BNI'), ('53', 'Dapatkan Cicilan 0% dengan Kartu Kredit BNI')]\n",
      "\n",
      "CIF: 2024253459 - Recommended Promo IDs: [('2', 'Dapatkan Penawaran Menarik dengan Kartu Debit dan QRIS Wondr by BNI'), ('5', 'wondr by BNI #Jadiinmaumu Tiap Hari Senin'), ('6', 'wondr by BNI #Jadiinmaumu Tiap Hari Senin'), ('8', 'Dapatkan Potongan hingga Rp100.000 dengan QRIS wondr by BNI'), ('9', 'Dapatkan Potongan Langsung Rp150.000 dengan QRIS wondr by BNI'), ('10', 'Dapatkan Potongan Langsung Rp150.000 dengan QRIS wondr by BNI'), ('11', 'Dapatkan Potongan Langsung Rp100.000 dengan QRIS wondr by BNI'), ('12', 'Dapatkan Potongan Langsung Rp100.000 dengan QRIS wondr by BNI'), ('13', 'Dapatkan Potongan Langsung Rp100.000 dengan QRIS wondr by BNI'), ('14', 'Buy 2 Get 3 Beverage dengan QRIS wondr by BNI')]\n",
      "\n",
      "CIF: 2022780533 - Recommended Promo IDs: [('54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot'), ('55', 'Servis Kendaraan di Honda Kencana Kranji bisa Semakin Hemat dengan Cicilan 0% s.d. 12 Bulan'), ('57', 'Diskon 20% hingga Rp15.000 dengan Kartu Kredit BNI'), ('61', 'Diskon hingga 90% untuk Pengguna Baru Grab dengan Kartu Kredit dan Debit BNI'), ('63', 'Dapatkan DIskon hingga Rp460.000 dengan Kartu Debit BNI'), ('64', 'Dapatkan Cicilan 0% hingga 12 Bulan dengan Kartu Kredit BNI'), ('65', 'Dapatkan DIskon 10% dengan Kartu BNI'), ('66', 'Dapatkan Diskon 7% dengan Kartu Kredit, Debit dan Debit BNI Emerald'), ('1', 'Dapatkan Potongan Rp100.000 dengan QRIS Wondr by BNI'), ('23', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu')]\n",
      "\n",
      "CIF: 2024474209 - Recommended Promo IDs: [('54', 'Cicilan 0,8% hingga 12 Bulan di Honda Daan Mogot'), ('55', 'Servis Kendaraan di Honda Kencana Kranji bisa Semakin Hemat dengan Cicilan 0% s.d. 12 Bulan'), ('57', 'Diskon 20% hingga Rp15.000 dengan Kartu Kredit BNI'), ('61', 'Diskon hingga 90% untuk Pengguna Baru Grab dengan Kartu Kredit dan Debit BNI'), ('63', 'Dapatkan DIskon hingga Rp460.000 dengan Kartu Debit BNI'), ('64', 'Dapatkan Cicilan 0% hingga 12 Bulan dengan Kartu Kredit BNI'), ('65', 'Dapatkan DIskon 10% dengan Kartu BNI'), ('66', 'Dapatkan Diskon 7% dengan Kartu Kredit, Debit dan Debit BNI Emerald'), ('18', 'Cahback Rp50.000 dengan QRIS wondr by BNI'), ('36', 'Diskon hingga 20% Tindakan Katarak dan Lasik dengan Kartu Kredit & Debit BNI')]\n",
      "\n",
      "CIF: 2024391823 - Recommended Promo IDs: [('1', 'Dapatkan Potongan Rp100.000 dengan QRIS Wondr by BNI'), ('23', 'wondr by BNI #Jadiinmaumu Tiap Hari Minggu'), ('24', 'wondr by BNI #Jadiinmaumu Tiap Hari Kamis'), ('27', 'Diskon hingga Rp150.000 di e-commerce Khusus Nasabah BNI di Hari Pelanggan 2024'), ('28', 'Dapatkan Diskon Rp150.000 dan Cicilan 0% hingga 24 bulan dengan Kartu Kredit BNI JCB'), ('29', 'Diskon Rp150.000 dengan Kartu Kredit BNI JCB'), ('30', 'Diskon Rp150.000 dengan Kartu Kredit BNI'), ('32', 'Dapatkan Diskon Rp75.000 dengan Kartu Kredit BNI JCB'), ('34', 'Dapatkan Voucher senilai Rp50.000 dengan Kartu Debit BNI'), ('35', 'Dapatkan Diskon Rp15.000 dengan Kartu Debit BNI')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Step 1: Create a pivot table (user-item interaction matrix)\n",
    "user_item_matrix = transaksi_df.pivot_table(index='cif', columns='category', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Save the columns (categories) and index (account IDs) for later use\n",
    "categories = user_item_matrix.columns\n",
    "accounts = user_item_matrix.index\n",
    "\n",
    "# Convert the interaction matrix to a NumPy array with a floating-point data type\n",
    "user_item_matrix = user_item_matrix.to_numpy(dtype=float)\n",
    "\n",
    "# Step 2: Determine the shape of the interaction matrix\n",
    "num_accounts, num_categories = user_item_matrix.shape\n",
    "\n",
    "# Set the value of k based on the shape of the matrix (k must be less than min(num_accounts, num_categories))\n",
    "k = min(num_accounts, num_categories) - 1\n",
    "\n",
    "# Step 3: Apply SVD to the user-item matrix\n",
    "U, sigma, Vt = svds(user_item_matrix, k=k)\n",
    "\n",
    "# Convert sigma to a diagonal matrix\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "# Step 4: Reconstruct the approximate interaction matrix\n",
    "predicted_matrix = np.dot(np.dot(U, sigma), Vt)\n",
    "\n",
    "# Step 5: Convert the predicted matrix back to a DataFrame for easier interpretation\n",
    "predicted_df = pd.DataFrame(predicted_matrix, index=accounts, columns=categories)\n",
    "\n",
    "# Step 6: Define the recommendation function\n",
    "def recommend_promo_hybrid(cif, num_recommendations=5):\n",
    "    # Get the predicted scores for the given account\n",
    "    user_predictions = predicted_df.loc[cif]\n",
    "    \n",
    "    # Sort merchants by predicted score in descending order\n",
    "    sorted_merchants = user_predictions.sort_values(ascending=False)\n",
    "    \n",
    "    # Get user's transaction data\n",
    "    user_transactions = transaksi_df[transaksi_df['cif'] == cif]\n",
    "    \n",
    "    # Initialize list to store recommendations with both promo IDs and names\n",
    "    recommendations = []\n",
    "\n",
    "    # First, recommend promos based on top predicted merchants\n",
    "    for merchant in sorted_merchants.index:\n",
    "        # Check for promos that match the current merchant\n",
    "        merchant_promos = promo_df[promo_df['merchant_name'] == merchant]\n",
    "        \n",
    "        if not merchant_promos.empty:\n",
    "            # If promos exist for the merchant, add both promo IDs and names to the recommendations\n",
    "            for _, promo in merchant_promos.iterrows():\n",
    "                recommendations.append((promo['id'], promo['promo_name']))  # Adjust 'id' if necessary\n",
    "                print(f\"Promo ID: {promo['id']}, Promo Name: {promo['promo_name']}\")\n",
    "                \n",
    "                # Break if we reach the number of recommendations\n",
    "                if len(recommendations) >= num_recommendations:\n",
    "                    return recommendations\n",
    "        \n",
    "        else:\n",
    "            # If no merchant-specific promos, fallback to check by category\n",
    "            user_merchant_transactions = user_transactions[user_transactions['merchant_name'] == merchant]\n",
    "            \n",
    "            if not user_merchant_transactions.empty:\n",
    "                user_category = user_merchant_transactions['category'].values[0]\n",
    "                \n",
    "                # Check if there are promos available for the same category\n",
    "                category_promos = promo_df[promo_df['category'] == user_category]\n",
    "                \n",
    "                if not category_promos.empty:\n",
    "                    for _, promo in category_promos.iterrows():\n",
    "                        recommendations.append((promo['id'], promo['promo_name']))  # Adjust 'id' if necessary\n",
    "                        # print(f\"Promo ID: {promo['id']}, Promo Name: {promo['promo_name']}\")\n",
    "                        \n",
    "                        # Break if we reach the number of recommendations\n",
    "                        if len(recommendations) >= num_recommendations:\n",
    "                            return recommendations\n",
    "\n",
    "    # If not enough merchant-based promos, fallback to recommending promos by transaction categories\n",
    "    for _, transaction in user_transactions.iterrows():\n",
    "        # Get promos for the transaction category\n",
    "        category_promos = promo_df[promo_df['category'] == transaction['category']]\n",
    "        \n",
    "        if not category_promos.empty:\n",
    "            for _, promo in category_promos.iterrows():\n",
    "                recommendations.append((promo['id'], promo['promo_name']))  # Adjust 'id' if necessary\n",
    "                # print(f\"Promo ID: {promo['id']}, Promo Name: {promo['promo_name']}\")\n",
    "                \n",
    "                # Break if we reach the number of recommendations\n",
    "                if len(recommendations) >= num_recommendations:\n",
    "                    return recommendations\n",
    "    \n",
    "    # Return recommendations up to the specified number\n",
    "    return recommendations[:num_recommendations]\n",
    "\n",
    "# Step 7: Generate recommendations for all unique CIFs\n",
    "unique_cifs = transaksi_df['cif'].unique()\n",
    "\n",
    "# Initialize a dictionary to store the recommendations for each CIF\n",
    "cif_recommendations = {}\n",
    "\n",
    "# Iterate through each unique CIF and generate recommendations\n",
    "for cif in unique_cifs:\n",
    "    # Generate recommendations for the current CIF\n",
    "    recommendations = recommend_promo_hybrid(cif, num_recommendations=10)\n",
    "    \n",
    "    # Store the recommendations in the dictionary\n",
    "    cif_recommendations[cif] = recommendations\n",
    "\n",
    "# Display the recommendations for each CIF\n",
    "for cif, recommendations in cif_recommendations.items():\n",
    "    print(f\"CIF: {cif} - Recommended Promo IDs: {recommendations}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Hybrid Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "def safe_divide(numerator, denominator):\n",
    "    return numerator / denominator if denominator != 0 else 0\n",
    "\n",
    "def precision_at_k(recommended_items, relevant_items, k):\n",
    "    if not recommended_items or not relevant_items:\n",
    "        return 0\n",
    "    recommended_set = set(recommended_items[:k])\n",
    "    relevant_set = set(relevant_items)\n",
    "    return safe_divide(len(recommended_set.intersection(relevant_set)), min(k, len(recommended_set)))\n",
    "\n",
    "def recall_at_k(recommended_items, relevant_items, k):\n",
    "    if not recommended_items or not relevant_items:\n",
    "        return 0\n",
    "    recommended_set = set(recommended_items[:k])\n",
    "    relevant_set = set(relevant_items)\n",
    "    return safe_divide(len(recommended_set.intersection(relevant_set)), len(relevant_set))\n",
    "\n",
    "def f1_at_k(recommended_items, relevant_items, k):\n",
    "    prec = precision_at_k(recommended_items, relevant_items, k)\n",
    "    rec = recall_at_k(recommended_items, relevant_items, k)\n",
    "    return safe_divide(2 * (prec * rec), (prec + rec))\n",
    "\n",
    "def ndcg_at_k(recommended_items, relevant_items, k):\n",
    "    if not recommended_items or not relevant_items:\n",
    "        return 0\n",
    "    relevance = np.array([1 if item in set(relevant_items) else 0 for item in recommended_items[:k]])\n",
    "    ideal_relevance = np.ones(min(k, len(relevant_items)))\n",
    "    \n",
    "    if len(relevance) < k:\n",
    "        relevance = np.pad(relevance, (0, k - len(relevance)))\n",
    "    if len(ideal_relevance) < k:\n",
    "        ideal_relevance = np.pad(ideal_relevance, (0, k - len(ideal_relevance)))\n",
    "    \n",
    "    try:\n",
    "        return ndcg_score([ideal_relevance], [relevance])\n",
    "    except ValueError:\n",
    "        return 0\n",
    "\n",
    "def average_precision(recommended_items, relevant_items):\n",
    "    if not recommended_items or not relevant_items:\n",
    "        return 0\n",
    "    hits = 0\n",
    "    sum_precs = 0\n",
    "    for n, item in enumerate(recommended_items, 1):\n",
    "        if item in relevant_items:\n",
    "            hits += 1\n",
    "            sum_precs += hits / n\n",
    "    return safe_divide(sum_precs, len(relevant_items))\n",
    "\n",
    "def evaluate_recommendations(recommendations, transaction_history, k_values=[1, 3, 5, 10]):\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        precisions, recalls, f1_scores, ndcg_scores, ap_scores = [], [], [], [], []\n",
    "        for user, recommended_items in recommendations.items():\n",
    "            relevant_items = transaction_history.get(user, [])\n",
    "            if not relevant_items:\n",
    "                print(f\"User {user} has no transaction history\")\n",
    "                continue\n",
    "            p = precision_at_k(recommended_items, relevant_items, k)\n",
    "            r = recall_at_k(recommended_items, relevant_items, k)\n",
    "            f1 = f1_at_k(recommended_items, relevant_items, k)\n",
    "            ndcg = ndcg_at_k(recommended_items, relevant_items, k)\n",
    "            ap = average_precision(recommended_items, relevant_items)\n",
    "            print(f\"User {user}: P@{k}={p:.2f}, R@{k}={r:.2f}, F1@{k}={f1:.2f}, NDCG@{k}={ndcg:.2f}, AP={ap:.2f}\")\n",
    "            precisions.append(p)\n",
    "            recalls.append(r)\n",
    "            f1_scores.append(f1)\n",
    "            ndcg_scores.append(ndcg)\n",
    "            ap_scores.append(ap)\n",
    "        \n",
    "        results[f'precision@{k}'] = np.mean(precisions) if precisions else 0\n",
    "        results[f'recall@{k}'] = np.mean(recalls) if recalls else 0\n",
    "        results[f'f1@{k}'] = np.mean(f1_scores) if f1_scores else 0\n",
    "        results[f'ndcg@{k}'] = np.mean(ndcg_scores) if ndcg_scores else 0\n",
    "    \n",
    "    results['MAP'] = np.mean(ap_scores) if ap_scores else 0\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_promos(cif, num_recommendations=5):\n",
    "    # Get the predicted scores for the given account\n",
    "    user_predictions = predicted_df.loc[cif]\n",
    "    \n",
    "    # Sort merchants by predicted score\n",
    "    sorted_merchants = user_predictions.sort_values(ascending=False)\n",
    "    print(sorted_merchants)\n",
    "    \n",
    "    # Return top N merchant names\n",
    "    # return sorted_merchants.index[:num_recommendations].tolist()\n",
    "    return sorted_merchants.index[:num_recommendations].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_recommendations = {}\n",
    "for user in accounts:\n",
    "    all_recommendations[user] = recommend_promos(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_history = {}\n",
    "for user in accounts:\n",
    "    user_transactions = transaksi_df[transaksi_df['cif'] == user]\n",
    "    transaction_history[user] = user_transactions['merchant_name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = evaluate_recommendations(all_recommendations, transaction_history)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in list(all_recommendations.keys())[:5]:\n",
    "    print(f\"User: {user}\")\n",
    "    print(f\"Recommendations: {all_recommendations[user][:5]}\")\n",
    "    print(f\"Transactions: {transaction_history.get(user, [])[:5]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_histories = sum(1 for transactions in transaction_history.values() if transactions)\n",
    "print(f\"Users with non-empty transaction histories: {non_empty_histories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_counts = [len(transactions) for transactions in transaction_history.values()]\n",
    "print(f\"Min transactions: {min(transaction_counts)}\")\n",
    "print(f\"Max transactions: {max(transaction_counts)}\")\n",
    "print(f\"Average transactions: {sum(transaction_counts) / len(transaction_counts):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Preprocess Data\n",
    "def preprocess_data(transaksi_df):\n",
    "    \"\"\"\n",
    "    Preprocess the transaction data, encode categorical columns.\n",
    "    \"\"\"\n",
    "    le_merchant = LabelEncoder()\n",
    "    le_cif = LabelEncoder()\n",
    "    \n",
    "    transaksi_df['merchant_encoded'] = le_merchant.fit_transform(transaksi_df['merchant_name'])\n",
    "    transaksi_df['cif_encoded'] = le_cif.fit_transform(transaksi_df['cif'])\n",
    "    \n",
    "    return transaksi_df, le_merchant, le_cif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Prepare Sequences for RNN\n",
    "def prepare_sequences(transaksi_df, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Prepare sequences of transactions for RNN training.\n",
    "    \"\"\"\n",
    "    transaksi_df = transaksi_df.sort_values(by=['cif', 'date'])\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for cif, group in transaksi_df.groupby('cif_encoded'):\n",
    "        merchant_list = group['merchant_encoded'].tolist()\n",
    "        \n",
    "        for i in range(len(merchant_list) - sequence_length):\n",
    "            sequences.append(merchant_list[i:i + sequence_length])\n",
    "            targets.append(merchant_list[i + sequence_length])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build RNN Model\n",
    "def buildModelRNN(num_merchants, sequence_length=20, embedding_dim=192):\n",
    "    \"\"\"\n",
    "    Build an RNN model for predicting the next merchant interaction.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_merchants, output_dim=embedding_dim, input_length=sequence_length))\n",
    "    model.add(LSTM(256, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_merchants, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModelAndSave(sequences, targets, num_merchants, model_filename='rnn_model_22092024.keras'):\n",
    "    \"\"\"\n",
    "    Train the RNN model using the prepared sequences and targets,\n",
    "    save the model to a .keras file, and return the model along with the train/test split data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Build the model\n",
    "        model = buildModelRNN(num_merchants=num_merchants)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "        \n",
    "        # Save the model to a .keras file (new Keras format)\n",
    "        model.save(model_filename)\n",
    "        print(f\"Model saved successfully to {model_filename}\")\n",
    "        \n",
    "        return model, X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while training or saving the model: {str(e)}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Load the RNN model from a .keras file and use it for recommendations\n",
    "def load_rnn_model(model_filename='rnn_model_22092024.keras'):\n",
    "    \"\"\"\n",
    "    Load the RNN model from a .keras file.\n",
    "    \"\"\"\n",
    "    return load_model(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Recommend Promos with a Higher Threshold and return promo IDs\n",
    "def recommend_promos_rnn(cif, model, transaksi_df, promo_df, le_merchant, le_cif, num_recommendations=30):\n",
    "    \"\"\"\n",
    "    Recommend promotions using the trained RNN model based on merchant and category,\n",
    "    and return only a list of promo IDs.\n",
    "    \"\"\"\n",
    "    # Encode the CIF (customer identification number)\n",
    "    cif_encoded = le_cif.transform([cif])[0]\n",
    "    user_transactions = transaksi_df[transaksi_df['cif_encoded'] == cif_encoded]\n",
    "    \n",
    "    # Sort the user's transactions by date and get recent interactions\n",
    "    user_transactions = user_transactions.sort_values(by='date')\n",
    "    recent_interactions = user_transactions['merchant_encoded'].values[-10:]\n",
    "    \n",
    "    # Predict the next merchant probabilities using the trained RNN model\n",
    "    next_merchant_probabilities = model.predict(np.array([recent_interactions]))\n",
    "    \n",
    "    # Get the top recommended merchants\n",
    "    recommended_merchants = np.argsort(next_merchant_probabilities[0])[::-1]\n",
    "    recommended_merchant_names = le_merchant.inverse_transform(recommended_merchants)\n",
    "    \n",
    "    # Store recommendations with associated scores\n",
    "    recommendations = {}\n",
    "    \n",
    "    # Step 7: Filter for both merchant and category promos, ensuring no duplicates\n",
    "    for idx, merchant in enumerate(recommended_merchant_names):\n",
    "        # Get the score for this merchant\n",
    "        score = next_merchant_probabilities[0][idx]\n",
    "        \n",
    "        # Check if promo exists for the predicted merchant\n",
    "        merchant_promos = promo_df[promo_df['merchant_name'] == merchant]\n",
    "        \n",
    "        if not merchant_promos.empty:\n",
    "            for promo_id in merchant_promos['id'].tolist():\n",
    "                if promo_id not in recommendations:\n",
    "                    recommendations[promo_id] = score\n",
    "        else:\n",
    "            # If no direct merchant promo, check for category-based promo\n",
    "            user_merchant_transactions = user_transactions[user_transactions['merchant_name'] == merchant]\n",
    "            if not user_merchant_transactions.empty:\n",
    "                user_category = user_merchant_transactions['category'].values[0]\n",
    "                \n",
    "                # Find promos based on category\n",
    "                category_promos = promo_df[promo_df['category'] == user_category]\n",
    "                if not category_promos.empty:\n",
    "                    for promo_id in category_promos['id'].tolist():\n",
    "                        if promo_id not in recommendations:\n",
    "                            # Reduce score for category-based recommendations\n",
    "                            recommendations[promo_id] = score * 0.5\n",
    "        \n",
    "        # Stop if we've collected enough recommendations\n",
    "        if len(recommendations) >= num_recommendations:\n",
    "            break\n",
    "    \n",
    "    # Step 8: Sort by recommendation strength (highest score first)\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract only the promo IDs from sorted recommendations\n",
    "    promo_ids = [promo_id for promo_id, _ in sorted_recommendations[:num_recommendations]]\n",
    "    \n",
    "    return promo_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/testEnv/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m415/415\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.1078 - loss: 4.1605 - val_accuracy: 0.1113 - val_loss: 4.1243\n",
      "Epoch 2/50\n",
      "\u001b[1m167/415\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.1163 - loss: 4.0874"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 4: Train the RNN model and save it to a .h5 file\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrnn_model_20240922.keras\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 15\u001b[0m rnn_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_rnn_model_and_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_merchants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_filename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Step 5: Load the model from the .h5 file for recommendations\u001b[39;00m\n\u001b[1;32m     18\u001b[0m rnn_model \u001b[38;5;241m=\u001b[39m load_rnn_model(model_filename)\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mtrain_rnn_model_and_save\u001b[0;34m(sequences, targets, num_merchants, model_filename)\u001b[0m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m build_rnn_model(num_merchants\u001b[38;5;241m=\u001b[39mnum_merchants)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Save the model to a .keras file (new Keras format)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39msave(model_filename)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/testEnv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/testEnv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/testEnv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/testEnv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:814\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;66;03m# Only count the statistics the first time, before initialization took\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# place.\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m   compiled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    816\u001b[0m                   \u001b[38;5;129;01mnot\u001b[39;00m control_flow_util\u001b[38;5;241m.\u001b[39mGraphOrParentsInXlaContext(\n\u001b[1;32m    817\u001b[0m                       ops\u001b[38;5;241m.\u001b[39mget_default_graph()))\n\u001b[1;32m    818\u001b[0m   \u001b[38;5;66;03m# For nested functions, increment the counter only when a function with\u001b[39;00m\n\u001b[1;32m    819\u001b[0m   \u001b[38;5;66;03m# jit_compile=True is called within a function with jit_compile=False. We\u001b[39;00m\n\u001b[1;32m    820\u001b[0m   \u001b[38;5;66;03m# count this special case to correctly record that both jit_compile=True\u001b[39;00m\n\u001b[1;32m    821\u001b[0m   \u001b[38;5;66;03m# and jit_compile=False is being used for parts of the outer function.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Step 2: Preprocess data\n",
    "    transaksi_df, le_merchant, le_cif = preprocess_data(transaksi_df)\n",
    "    \n",
    "    # Step 3: Prepare sequences\n",
    "    sequences, targets = prepare_sequences(transaksi_df)\n",
    "    \n",
    "    # Get the number of unique merchants\n",
    "    num_merchants = len(le_merchant.classes_)\n",
    "    \n",
    "    # Step 4: Train the RNN model and save it to a .h5 file\n",
    "    model_filename = 'rnn_model_20240922.keras'\n",
    "    rnn_model = trainModelAndSave(sequences, targets, num_merchants, model_filename=model_filename)\n",
    "    \n",
    "    # Step 5: Load the model from the .h5 file for recommendations\n",
    "    rnn_model = load_rnn_model(model_filename)\n",
    "    \n",
    "    # Get recommendations for a specific user\n",
    "    recommendations = recommend_promos_rnn(\n",
    "        cif=\"2024391823\", \n",
    "        model=rnn_model, \n",
    "        transaksi_df=transaksi_df, \n",
    "        promo_df=promo_df, \n",
    "        le_merchant=le_merchant, \n",
    "        le_cif=le_cif,\n",
    "        num_recommendations=30,  \n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"Recommended Promo IDs:\", recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = trainModelAndSave(sequences, targets, num_merchants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 2: Preprocess Data\n",
    "def preprocess_data(transaksi_df):\n",
    "    \"\"\"\n",
    "    Preprocess the transaction data, encode categorical columns.\n",
    "    \"\"\"\n",
    "    le_merchant = LabelEncoder()\n",
    "    le_cif = LabelEncoder()\n",
    "    \n",
    "    transaksi_df['merchant_encoded'] = le_merchant.fit_transform(transaksi_df['merchant_name'])\n",
    "    transaksi_df['cif_encoded'] = le_cif.fit_transform(transaksi_df['cif'])\n",
    "    \n",
    "    return transaksi_df, le_merchant, le_cif\n",
    "\n",
    "# Step 3: Prepare Sequences for RNN\n",
    "def prepare_sequences(transaksi_df, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Prepare sequences of transactions for RNN training.\n",
    "    \"\"\"\n",
    "    transaksi_df = transaksi_df.sort_values(by=['cif', 'date'])\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for cif, group in transaksi_df.groupby('cif_encoded'):\n",
    "        merchant_list = group['merchant_encoded'].tolist()\n",
    "        \n",
    "        for i in range(len(merchant_list) - sequence_length):\n",
    "            sequences.append(merchant_list[i:i + sequence_length])\n",
    "            targets.append(merchant_list[i + sequence_length])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Step 4: Build RNN Model\n",
    "def buildModelRNN(num_merchants, sequence_length=20, embedding_dim=192):\n",
    "    \"\"\"\n",
    "    Build an RNN model for predicting the next merchant interaction.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_merchants, output_dim=embedding_dim, input_length=sequence_length))\n",
    "    model.add(LSTM(256, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_merchants, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def trainModelAndSave(sequences, targets, num_merchants, model_filename='rnn_model_22092024.keras'):\n",
    "    \"\"\"\n",
    "    Train the RNN model using the prepared sequences and targets,\n",
    "    save the model to a .keras file, and return the model along with the train/test split data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Build the model\n",
    "        model = buildModelRNN(num_merchants=num_merchants)\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))\n",
    "        \n",
    "        # Save the model to a .keras file (new Keras format)\n",
    "        model.save(model_filename)\n",
    "        print(f\"Model saved successfully to {model_filename}\")\n",
    "        \n",
    "        return model, X_train, X_test, y_train, y_test\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while training or saving the model: {str(e)}\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Load the RNN model from a .keras file and use it for recommendations\n",
    "def load_rnn_model(model_filename='rnn_model_22092024.keras'):\n",
    "    \"\"\"\n",
    "    Load the RNN model from a .keras file.\n",
    "    \"\"\"\n",
    "    return load_model(model_filename)\n",
    "\n",
    "# Step 6: Recommend Promos with a Higher Threshold and return promo IDs\n",
    "def recommend_promos_rnn(cif, model, transaksi_df, promo_df, le_merchant, le_cif, num_recommendations=30):\n",
    "    \"\"\"\n",
    "    Recommend promotions using the trained RNN model based on merchant and category,\n",
    "    and return only a list of promo IDs.\n",
    "    \"\"\"\n",
    "    # Encode the CIF (customer identification number)\n",
    "    cif_encoded = le_cif.transform([cif])[0]\n",
    "    user_transactions = transaksi_df[transaksi_df['cif_encoded'] == cif_encoded]\n",
    "    \n",
    "    # Sort the user's transactions by date and get recent interactions\n",
    "    user_transactions = user_transactions.sort_values(by='date')\n",
    "    recent_interactions = user_transactions['merchant_encoded'].values[-10:]\n",
    "    \n",
    "    # Predict the next merchant probabilities using the trained RNN model\n",
    "    next_merchant_probabilities = model.predict(np.array([recent_interactions]))\n",
    "    \n",
    "    # Get the top recommended merchants\n",
    "    recommended_merchants = np.argsort(next_merchant_probabilities[0])[::-1]\n",
    "    recommended_merchant_names = le_merchant.inverse_transform(recommended_merchants)\n",
    "    \n",
    "    # Store recommendations with associated scores\n",
    "    recommendations = {}\n",
    "    \n",
    "    # Step 7: Filter for both merchant and category promos, ensuring no duplicates\n",
    "    for idx, merchant in enumerate(recommended_merchant_names):\n",
    "        # Get the score for this merchant\n",
    "        score = next_merchant_probabilities[0][idx]\n",
    "        \n",
    "        # Check if promo exists for the predicted merchant\n",
    "        merchant_promos = promo_df[promo_df['merchant_name'] == merchant]\n",
    "        \n",
    "        if not merchant_promos.empty:\n",
    "            for promo_id in merchant_promos['id'].tolist():\n",
    "                if promo_id not in recommendations:\n",
    "                    recommendations[promo_id] = score\n",
    "        else:\n",
    "            # If no direct merchant promo, check for category-based promo\n",
    "            user_merchant_transactions = user_transactions[user_transactions['merchant_name'] == merchant]\n",
    "            if not user_merchant_transactions.empty:\n",
    "                user_category = user_merchant_transactions['category'].values[0]\n",
    "                \n",
    "                # Find promos based on category\n",
    "                category_promos = promo_df[promo_df['category'] == user_category]\n",
    "                if not category_promos.empty:\n",
    "                    for promo_id in category_promos['id'].tolist():\n",
    "                        if promo_id not in recommendations:\n",
    "                            # Reduce score for category-based recommendations\n",
    "                            recommendations[promo_id] = score * 0.5\n",
    "        \n",
    "        # Stop if we've collected enough recommendations\n",
    "        if len(recommendations) >= num_recommendations:\n",
    "            break\n",
    "    \n",
    "    # Step 8: Sort by recommendation strength (highest score first)\n",
    "    sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Extract only the promo IDs from sorted recommendations\n",
    "    promo_ids = [promo_id for promo_id, _ in sorted_recommendations[:num_recommendations]]\n",
    "    \n",
    "    return promo_ids\n",
    "\n",
    "# Step 9: Recommend Promos per CIF\n",
    "def recommend_promo_rnn(transaksi_df, promo_df, model, le_merchant, le_cif, num_recommendations=30):\n",
    "    \"\"\"\n",
    "    Recommend promos for each unique CIF.\n",
    "    \"\"\"\n",
    "    unique_cifs = transaksi_df['cif'].unique()\n",
    "    all_recommendations = {}\n",
    "\n",
    "    for cif in unique_cifs:\n",
    "        print(f\"Generating recommendations for CIF: {cif}\")\n",
    "        promo_ids = recommend_promos_rnn(cif, model, transaksi_df, promo_df, le_merchant, le_cif, num_recommendations)\n",
    "        all_recommendations[cif] = promo_ids\n",
    "\n",
    "    return all_recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations for CIF: 2023188761\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "Generating recommendations for CIF: 2023433417\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Generating recommendations for CIF: 2022734196\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Generating recommendations for CIF: 2024688533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Generating recommendations for CIF: 2022322084\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Generating recommendations for CIF: 2024504637\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Generating recommendations for CIF: 2024253459\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Generating recommendations for CIF: 2022780533\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Generating recommendations for CIF: 2024474209\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Generating recommendations for CIF: 2024391823\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "CIF: 2023188761, Recommended Promo IDs: ['61', '26', '22', '46', '39', '12', '45', '40', '3', '18', '10', '44', '60', '58', '23', '50', '43', '55', '47', '59', '25', '7', '51', '56', '29', '54', '31', '32', '16', '28']\n",
      "CIF: 2023433417, Recommended Promo IDs: ['16', '7', '5', '40', '27', '36', '32', '64', '49', '46', '28', '55', '18', '19', '47', '51', '25', '17', '60', '37', '23', '42', '39', '13', '35', '2', '45', '30', '10', '29']\n",
      "CIF: 2022734196, Recommended Promo IDs: ['7', '28', '52', '14', '46', '43', '31', '55', '25', '17', '56', '60', '29', '30', '47', '27', '10', '20', '12', '5', '58', '42', '18', '45', '19', '50', '64', '51', '37', '32']\n",
      "CIF: 2024688533, Recommended Promo IDs: ['39', '64', '45', '7', '63', '61', '25', '35', '59', '9', '56', '20', '36', '51', '58', '4', '29', '46', '60', '5', '31', '42', '27', '28', '10', '47', '50', '32', '49', '55']\n",
      "CIF: 2022322084, Recommended Promo IDs: ['26', '36', '20', '39', '58', '29', '38', '59', '60', '10', '54', '55', '51', '43', '27', '64', '49', '17', '18', '5', '35', '40', '61', '32', '42', '7', '56', '63', '47', '46']\n",
      "CIF: 2024504637, Recommended Promo IDs: ['41', '31', '64', '23', '20', '10', '55', '39', '28', '27', '7', '47', '29', '51', '36', '46', '17', '45', '25', '58', '42', '18', '35', '60', '49', '32', '5', '38', '37', '30']\n",
      "CIF: 2024253459, Recommended Promo IDs: ['40', '45', '46', '51', '20', '7', '26', '5', '19', '37', '29', '43', '60', '30', '28', '27', '31', '47', '34', '17', '64', '55', '49', '25', '32', '12', '22', '58', '10', '18']\n",
      "CIF: 2022780533, Recommended Promo IDs: ['11', '17', '50', '26', '14', '62', '46', '10', '32', '2', '7', '64', '42', '8', '55', '41', '57', '45', '31', '40', '4', '34', '20', '51', '43', '22', '63', '30', '24', '29']\n",
      "CIF: 2024474209, Recommended Promo IDs: ['65', '12', '44', '53', '63', '36', '18', '43', '15', '16', '7', '38', '56', '27', '61', '26', '3', '58', '35', '64', '51', '2', '47', '9', '60', '21', '59', '49', '52', '39']\n",
      "CIF: 2024391823, Recommended Promo IDs: ['64', '52', '20', '29', '30', '10', '26', '53', '23', '62', '58', '7', '51', '18', '41', '59', '40', '32', '48', '60', '11', '17', '55', '43', '31', '45', '34', '24', '42', '27']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data and encode merchants and CIFs\n",
    "transaksi_df, le_merchant, le_cif = preprocess_data(transaksi_df)\n",
    "\n",
    "# Load the pre-trained RNN model\n",
    "model = load_rnn_model('rnn_model_22092024.keras')\n",
    "\n",
    "# Generate recommendations per CIF\n",
    "recommendations_per_cif = recommend_promo_rnn(transaksi_df, promo_df, model, le_merchant, le_cif, num_recommendations=30)\n",
    "\n",
    "# Print the recommendations\n",
    "for cif, promo_ids in recommendations_per_cif.items():\n",
    "    print(f\"CIF: {cif}, Recommended Promo IDs: {promo_ids}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Step 2: Preprocess Data\n",
    "def preprocess_data(transaksi_df):\n",
    "    \"\"\"\n",
    "    Preprocess the transaction data, encode categorical columns.\n",
    "    \"\"\"\n",
    "    le_merchant = LabelEncoder()\n",
    "    le_cif = LabelEncoder()\n",
    "    \n",
    "    transaksi_df['merchant_encoded'] = le_merchant.fit_transform(transaksi_df['merchant_name'])\n",
    "    transaksi_df['cif_encoded'] = le_cif.fit_transform(transaksi_df['cif'])\n",
    "    \n",
    "    return transaksi_df, le_merchant, le_cif\n",
    "\n",
    "# Step 3: Prepare Sequences for RNN\n",
    "def prepare_sequences(transaksi_df, sequence_length=10):\n",
    "    \"\"\"\n",
    "    Prepare sequences of transactions for RNN training.\n",
    "    \"\"\"\n",
    "    transaksi_df = transaksi_df.sort_values(by=['cif', 'date'])\n",
    "    \n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    for cif, group in transaksi_df.groupby('cif_encoded'):\n",
    "        merchant_list = group['merchant_encoded'].tolist()\n",
    "        \n",
    "        for i in range(len(merchant_list) - sequence_length):\n",
    "            sequences.append(merchant_list[i:i + sequence_length])\n",
    "            targets.append(merchant_list[i + sequence_length])\n",
    "    \n",
    "    return np.array(sequences), np.array(targets)\n",
    "    \n",
    "def top_n_accuracy(y_true, y_pred, n=5):\n",
    "    \"\"\"\n",
    "    Calculate the Top-N accuracy.\n",
    "    \"\"\"\n",
    "    return np.mean([1 if true in pred[:n] else 0 for true, pred in zip(y_true, y_pred)])\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Reciprocal Rank (MRR).\n",
    "    \"\"\"\n",
    "    ranks = [np.where(pred == true)[0][0] + 1 if true in pred else 0 for true, pred in zip(y_true, y_pred)]\n",
    "    return np.mean([1 / rank if rank != 0 else 0 for rank in ranks])\n",
    "\n",
    "def ndcg_at_k(y_true, y_pred, k=5):\n",
    "    \"\"\"\n",
    "    Calculate the Normalized Discounted Cumulative Gain (NDCG) at k.\n",
    "    \"\"\"\n",
    "    def dcg_at_k(r, k):\n",
    "        r = np.asfarray(r)[:k]\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "\n",
    "    scores = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        relevance = [1 if item == true else 0 for item in pred[:k]]\n",
    "        ideal = sorted(relevance, reverse=True)\n",
    "        dcg = dcg_at_k(relevance, k)\n",
    "        idcg = dcg_at_k(ideal, k)\n",
    "        scores.append(dcg / idcg if idcg > 0 else 0)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def hit_rate(y_true, y_pred, k=5):\n",
    "    \"\"\"\n",
    "    Calculate the Hit Rate at k.\n",
    "    \"\"\"\n",
    "    return np.mean([1 if true in pred[:k] else 0 for true, pred in zip(y_true, y_pred)])\n",
    "\n",
    "def evaluate_rnn_model(model, X_test, y_test, top_n=5):\n",
    "    \"\"\"\n",
    "    Evaluate the RNN model using various metrics suitable for recommendation systems.\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = np.argsort(y_pred_proba, axis=1)[:, ::-1]  # Sort predictions in descending order\n",
    "\n",
    "    # Calculate metrics\n",
    "    top_n_acc = top_n_accuracy(y_test, y_pred, n=top_n)\n",
    "    mrr = mean_reciprocal_rank(y_test, y_pred)\n",
    "    ndcg = ndcg_at_k(y_test, y_pred, k=top_n)\n",
    "    hr = hit_rate(y_test, y_pred, k=top_n)\n",
    "\n",
    "    print(f\"Top-{top_n} Accuracy: {top_n_acc:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank: {mrr:.4f}\")\n",
    "    print(f\"NDCG@{top_n}: {ndcg:.4f}\")\n",
    "    print(f\"Hit Rate@{top_n}: {hr:.4f}\")\n",
    "\n",
    "    return {\n",
    "        f\"top_{top_n}_accuracy\": top_n_acc,\n",
    "        \"mean_reciprocal_rank\": mrr,\n",
    "        f\"ndcg@{top_n}\": ndcg,\n",
    "        f\"hit_rate@{top_n}\": hr\n",
    "    }\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming you have your transaction DataFrame ready\n",
    "    transaksi_df, le_merchant, le_cif = preprocess_data(transaksi_df)  # Reuse your preprocessing\n",
    "\n",
    "    # Prepare sequences\n",
    "    sequences, targets = prepare_sequences(transaksi_df)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Load the saved model\n",
    "    model = load_model('best_rnn_recommendation_model.keras')\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluation_results = evaluate_rnn_model(model, X_test, y_test, top_n=5)\n",
    "\n",
    "    # Print detailed results\n",
    "    print(\"\\nDetailed Evaluation Results:\")\n",
    "    for metric, value in evaluation_results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HYPERPARAMETER TUNING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import sqlalchemy  # For database connection\n",
    "\n",
    "def load_data(conn):\n",
    "    \"\"\"\n",
    "    Load the transaction data from the database, filtering for 'status_transaction = D'.\n",
    "    \"\"\"\n",
    "    query = '''\n",
    "    SELECT * FROM transactions WHERE status_transaction = 'D';\n",
    "    '''\n",
    "    transaksi_df = pd.read_sql_query(query, conn)\n",
    "    return transaksi_df\n",
    "\n",
    "def build_model(hp, num_merchants, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=num_merchants,\n",
    "        output_dim=hp.Int('embedding_dim', min_value=32, max_value=256, step=32),\n",
    "        input_length=sequence_length\n",
    "    ))\n",
    "    model.add(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=512, step=32),\n",
    "        return_sequences=hp.Boolean('return_sequences')\n",
    "    ))\n",
    "    \n",
    "    # Add an optional second LSTM layer\n",
    "    if hp.Boolean('use_second_lstm'):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Int('lstm_units_2', min_value=32, max_value=512, step=32),\n",
    "            return_sequences=False\n",
    "        ))\n",
    "    elif hp.Boolean('return_sequences'):\n",
    "        model.add(LSTM(32, return_sequences=False))\n",
    "    \n",
    "    model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(num_merchants, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tune_hyperparameters(X_train, y_train, num_merchants, sequence_length):\n",
    "    tuner = RandomSearch(\n",
    "        lambda hp: build_model(hp, num_merchants, sequence_length),\n",
    "        objective='val_accuracy',\n",
    "        max_trials=50,  # Increase this for a more exhaustive search\n",
    "        executions_per_trial=2,\n",
    "        directory='rnn_tuning',\n",
    "        project_name='merchant_recommendation'\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        X_train, y_train,\n",
    "        epochs=30,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)]\n",
    "    )\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    return best_hps, tuner\n",
    "\n",
    "def main():\n",
    "    transaksi_df, le_merchant, le_cif = preprocess_data(transaksi_df)\n",
    "    \n",
    "    # Prepare sequences\n",
    "    sequences, targets = prepare_sequences(transaksi_df)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
    "    \n",
    "    num_merchants = len(le_merchant.classes_)\n",
    "    sequence_length = X_train.shape[1]\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    best_hps, tuner = tune_hyperparameters(X_train, y_train, num_merchants, sequence_length)\n",
    "    \n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for param, value in best_hps.values.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    # Build and train the model with best hyperparameters\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    history = best_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,  # You may want to adjust this\n",
    "        validation_split=0.2,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=10)]\n",
    "    )\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "import sqlalchemy\n",
    "\n",
    "def load_data(conn):\n",
    "    \"\"\"\n",
    "    Load the transaction and promo data from the database.\n",
    "    \"\"\"\n",
    "    query_trans = \"SELECT * FROM transactions WHERE status_transaction = 'D';\"\n",
    "    query_promo = \"SELECT * FROM promo;\"\n",
    "    \n",
    "    transaksi_df = pd.read_sql_query(query_trans, conn)\n",
    "    promo_df = pd.read_sql_query(query_promo, conn)\n",
    "    \n",
    "    return transaksi_df, promo_df\n",
    "\n",
    "def match_promotions(transaksi_df, promo_df):\n",
    "    \"\"\"\n",
    "    Match the promotions based on the user's transactions, considering both merchant and category.\n",
    "    \"\"\"\n",
    "    # Merge transactions and promo data on 'merchant_name' and 'category'\n",
    "    merged_df = pd.merge(transaksi_df, promo_df, how='inner', on=['merchant_name', 'category'])\n",
    "    \n",
    "    # Filter out unnecessary columns if needed\n",
    "    return merged_df\n",
    "\n",
    "def build_model(hp, num_merchants, sequence_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=num_merchants,\n",
    "        output_dim=hp.Int('embedding_dim', min_value=32, max_value=256, step=32),\n",
    "        input_length=sequence_length\n",
    "    ))\n",
    "    model.add(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=512, step=32),\n",
    "        return_sequences=hp.Boolean('return_sequences')\n",
    "    ))\n",
    "    \n",
    "    # Add an optional second LSTM layer\n",
    "    if hp.Boolean('use_second_lstm'):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Int('lstm_units_2', min_value=32, max_value=512, step=32),\n",
    "            return_sequences=False\n",
    "        ))\n",
    "    elif hp.Boolean('return_sequences'):\n",
    "        model.add(LSTM(32, return_sequences=False))\n",
    "    \n",
    "    model.add(Dropout(hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(num_merchants, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def tune_hyperparameters(X_train, y_train, num_merchants, sequence_length):\n",
    "    tuner = RandomSearch(\n",
    "        lambda hp: build_model(hp, num_merchants, sequence_length),\n",
    "        objective='val_accuracy',\n",
    "        max_trials=200,\n",
    "        executions_per_trial=2,\n",
    "        directory='rnn_tuning',\n",
    "        project_name='merchant_recommendation'\n",
    "    )\n",
    "\n",
    "    tuner.search(\n",
    "        X_train, y_train,\n",
    "        epochs=200,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=5)]\n",
    "    )\n",
    "\n",
    "    best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    return best_hps, tuner\n",
    "\n",
    "def preprocess_data(transaksi_df, promo_df):\n",
    "    \"\"\"\n",
    "    Preprocess the transaction data and the promo data. Use LabelEncoder for categorical features.\n",
    "    \"\"\"\n",
    "    # Encoding 'merchant_name' and 'category'\n",
    "    le_merchant = LabelEncoder()\n",
    "    le_cif = LabelEncoder()\n",
    "    \n",
    "    transaksi_df['merchant_name'] = le_merchant.fit_transform(transaksi_df['merchant_name'])\n",
    "    transaksi_df['cif_encoded'] = le_cif.fit_transform(transaksi_df['cif'])\n",
    "\n",
    "    return transaksi_df, le_merchant, le_cif\n",
    "\n",
    "def prepare_sequences(transaksi_df):\n",
    "    \"\"\"\n",
    "    Prepare sequences and targets for LSTM.\n",
    "    \"\"\"\n",
    "    # Assuming that 'merchant_id' will be the input feature\n",
    "    sequences = transaksi_df[['merchant_name']].values\n",
    "    targets = transaksi_df['merchant_name'].shift(-1).fillna(0).astype(int)  # Shift for next merchant\n",
    "    \n",
    "    return sequences, targets\n",
    "\n",
    "def main():\n",
    "    # Create a connection to the PostgreSQL database using SQLAlchemy\n",
    "    # db_url = \"postgresql+psycopg2://username:password@localhost:5432/dbname\"\n",
    "    # conn = sqlalchemy.create_engine(db_url).connect()\n",
    "    \n",
    "    # Load transaction and promo data\n",
    "    transaksi_df, promo_df = load_data(conn)\n",
    "    \n",
    "    # Match transactions with promotions\n",
    "    merged_df = match_promotions(transaksi_df, promo_df)\n",
    "    \n",
    "    # Preprocess data\n",
    "    merged_df, le_merchant, le_cif = preprocess_data(merged_df, promo_df)\n",
    "    \n",
    "    # Prepare sequences\n",
    "    sequences, targets = prepare_sequences(merged_df)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(sequences, targets, test_size=0.2, random_state=42)\n",
    "    \n",
    "    num_merchants = len(le_merchant.classes_)\n",
    "    sequence_length = X_train.shape[1]\n",
    "    \n",
    "    # Tune hyperparameters\n",
    "    best_hps, tuner = tune_hyperparameters(X_train, y_train, num_merchants, sequence_length)\n",
    "    \n",
    "    print(\"Best Hyperparameters:\")\n",
    "    for param, value in best_hps.values.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    \n",
    "    # Build and train the model with best hyperparameters\n",
    "    best_model = tuner.hypermodel.build(best_hps)\n",
    "    history = best_model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,  # You may want to adjust this\n",
    "        validation_split=0.2,\n",
    "        callbacks=[keras.callbacks.EarlyStopping('val_loss', patience=10)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_loss, test_accuracy = best_model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_accuracy}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    best_model.save('best_rnn_recommendation_model.keras')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRAPH NEURAL NETWORK**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "# Load the data\n",
    "def load_data(conn):\n",
    "    query = '''SELECT * FROM transactions WHERE status_transaction = 'D';'''\n",
    "    transaksi_df = pd.read_sql_query(query, conn)\n",
    "    \n",
    "    promo_query = '''SELECT * FROM promo;'''\n",
    "    promo_df = pd.read_sql_query(promo_query, conn)\n",
    "    \n",
    "    return transaksi_df, promo_df\n",
    "\n",
    "def create_graph(transaksi_df, promo_df):\n",
    "    # Get unique user IDs and promo IDs\n",
    "    user_ids = transaksi_df['cif'].unique()\n",
    "    promo_ids = promo_df['promo_name'].unique()\n",
    "\n",
    "    # Create a mapping from user IDs and promo IDs to indices\n",
    "    user_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n",
    "    promo_index = {promo_id: idx + len(user_ids) for idx, promo_id in enumerate(promo_ids)}\n",
    "\n",
    "    # Initialize feature tensor with appropriate dimensions\n",
    "    user_features = torch.eye(len(user_ids))  # One-hot encoding for users\n",
    "    promo_features = torch.eye(len(promo_ids))  # One-hot encoding for promotions\n",
    "    x = torch.cat([user_features, promo_features], dim=0)  # Concatenate user and promo features\n",
    "\n",
    "    edge_index = []\n",
    "\n",
    "    # Create edges based on transactions and matching promotions\n",
    "    for index, row in transaksi_df.iterrows():\n",
    "        user_cif = row['cif']\n",
    "        merchant_name = row['merchant_name']\n",
    "        category = row['category']\n",
    "\n",
    "        if user_cif in user_index:\n",
    "            user_idx = user_index[user_cif]\n",
    "\n",
    "            # Find matching promotions\n",
    "            matching_promos = promo_df[\n",
    "                (promo_df['merchant_name'] == merchant_name) &\n",
    "                (promo_df['category'] == category)\n",
    "            ]\n",
    "\n",
    "            for _, promo in matching_promos.iterrows():\n",
    "                promo_name = promo['promo_name']\n",
    "                if promo_name in promo_index:\n",
    "                    promo_idx = promo_index[promo_name]\n",
    "                    edge_index.append([user_idx, promo_idx])  # Connect user to promo\n",
    "\n",
    "    # Convert edge_index to a tensor\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    return Data(x=x, edge_index=edge_index)\n",
    "\n",
    "\n",
    "# Define the GNN model\n",
    "class GNNModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_units):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_units)\n",
    "        self.conv2 = GCNConv(hidden_units, hidden_units)\n",
    "        self.fc = torch.nn.Linear(hidden_units, 1)  # Output for recommendation score\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_units = trial.suggest_int(\"hidden_units\", 16, 128)\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
    "\n",
    "    # Create a connection to the PostgreSQL database\n",
    "    # db_url = \"postgresql+psycopg2://username:password@localhost:5432/dbname\"\n",
    "    # conn = sqlalchemy.create_engine(db_url).connect()\n",
    "\n",
    "    # Load data\n",
    "    transaksi_df, promo_df = load_data(conn)\n",
    "\n",
    "    # Create graph\n",
    "    graph_data = create_graph(transaksi_df, promo_df)\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = GNNModel(num_features=graph_data.x.shape[1], hidden_units=hidden_units)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for epoch in range(100):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(graph_data)\n",
    "\n",
    "        # Use only the prediction corresponding to the promo nodes\n",
    "        promo_start_idx = len(transaksi_df['cif'].unique())\n",
    "        promo_outputs = out[promo_start_idx:]  # Get outputs for promotions\n",
    "\n",
    "        # Create dummy targets (this should be replaced with real targets)\n",
    "        # Ensure the length matches promo_outputs\n",
    "        targets = torch.ones(promo_outputs.shape)  # Adjust as needed\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = F.mse_loss(promo_outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(graph_data)\n",
    "        promo_outputs = predictions[promo_start_idx:]\n",
    "        accuracy = ((promo_outputs > 0.5).float() == torch.ones(promo_outputs.shape)).float().mean().item()\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), f\"gnn_model_trial_{trial.number}.pt\")\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def main():\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=50)\n",
    "\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "    # Load and evaluate the best model\n",
    "    best_model = GNNModel(num_features=graph_data.x.shape[1], hidden_units=study.best_params[\"hidden_units\"])\n",
    "    best_model.load_state_dict(torch.load(f\"gnn_model_trial_{study.best_trial.number}.pt\"))\n",
    "\n",
    "    # Evaluate the best model\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = best_model(graph_data)\n",
    "        promo_outputs = predictions[len(transaksi_df['cif'].unique()):]\n",
    "        accuracy = ((promo_outputs > 0.5).float() == torch.ones(promo_outputs.shape)).float().mean().item()\n",
    "    \n",
    "    print(f\"Best Model Accuracy: {accuracy}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature tensor shape:\", x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "\n",
    "# Define a function to build the model\n",
    "def buildModelRNN(embedding_dim=50, lstm_units=64, dropout_rate=0.2):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=num_merchants, output_dim=embedding_dim, input_length=sequence_length))\n",
    "    model.add(LSTM(lstm_units, return_sequences=False))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(num_merchants, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the model in KerasClassifier\n",
    "model = KerasClassifier(build_fn=buildModelRNN, verbose=0)\n",
    "\n",
    "# Define the grid of hyperparameters\n",
    "param_grid = {\n",
    "    'embedding_dim': [50, 100],\n",
    "    'lstm_units': [32, 64],\n",
    "    'dropout_rate': [0.2, 0.3],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [50, 100]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_result = grid.fit(sequences, targets)\n",
    "\n",
    "# Print the best score and hyperparameters\n",
    "print(f\"Best score: {grid_result.best_score_}\")\n",
    "print(f\"Best hyperparameters: {grid_result.best_params_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "# Load transaction data\n",
    "transaksi_df = pd.read_sql_query('''\n",
    "    SELECT id, cif, amount, \"date\", description, category, merchant_name, status_transaction, account_number\n",
    "    FROM public.transactions\n",
    "''', conn)\n",
    "\n",
    "# Load promo data\n",
    "promo_df = pd.read_sql_query('''\n",
    "    SELECT id, promo_name, category, merchant_name, startdate, enddate\n",
    "    FROM public.promo\n",
    "''', conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Preprocess the data\n",
    "le_merchant = LabelEncoder()\n",
    "le_category = LabelEncoder()\n",
    "le_promo = LabelEncoder()\n",
    "\n",
    "transaksi_df['merchant_id'] = le_merchant.fit_transform(transaksi_df['merchant_name'])\n",
    "transaksi_df['category'] = le_category.fit_transform(transaksi_df['category'])\n",
    "promo_df['promo_id'] = le_promo.fit_transform(promo_df['promo_name'])\n",
    "\n",
    "# Convert date strings to datetime objects\n",
    "transaksi_df['date'] = pd.to_datetime(transaksi_df['date'])\n",
    "promo_df['startdate'] = pd.to_datetime(promo_df['startdate'])\n",
    "promo_df['enddate'] = pd.to_datetime(promo_df['enddate'])\n",
    "\n",
    "# Create features from transaction data\n",
    "def create_user_features(df):\n",
    "    user_features = df.groupby('cif').agg({\n",
    "        'amount': ['mean', 'sum', 'count'],\n",
    "        'merchant_id': 'nunique',\n",
    "        'category': 'nunique',\n",
    "        'date': ['min', 'max']\n",
    "    })\n",
    "    user_features.columns = ['avg_transaction', 'total_spent', 'transaction_count', 'unique_merchants', 'unique_categories', 'first_transaction', 'last_transaction']\n",
    "    user_features['account_age_days'] = (user_features['last_transaction'] - user_features['first_transaction']).dt.days\n",
    "    return user_features.reset_index()\n",
    "\n",
    "user_features = create_user_features(transaksi_df)\n",
    "\n",
    "# Create a function to match transactions with promos\n",
    "def match_transactions_with_promos(transactions, promos):\n",
    "    matched_data = []\n",
    "    for _, transaction in transactions.iterrows():\n",
    "        for _, promo in promos.iterrows():\n",
    "            if (transaction['merchant_name'] == promo['merchant_name'] and\n",
    "                transaction['category'] == promo['category'] and\n",
    "                promo['startdate'] <= transaction['date'] <= promo['enddate']):\n",
    "                matched_data.append({\n",
    "                    'cif': transaction['cif'],\n",
    "                    'promo_id': promo['promo_id'],\n",
    "                    'used_promo': 1\n",
    "                })\n",
    "    return pd.DataFrame(matched_data)\n",
    "\n",
    "# Match transactions with promos\n",
    "user_promo_data = match_transactions_with_promos(transaksi_df, promo_df)\n",
    "\n",
    "# Add negative samples (promos not used)\n",
    "all_users = set(transaksi_df['cif'])\n",
    "all_promos = set(promo_df['promo_id'])\n",
    "negative_samples = []\n",
    "for user in all_users:\n",
    "    unused_promos = all_promos - set(user_promo_data[user_promo_data['cif'] == user]['promo_id'])\n",
    "    for promo in unused_promos:\n",
    "        negative_samples.append({\n",
    "            'cif': user,\n",
    "            'promo_id': promo,\n",
    "            'used_promo': 0\n",
    "        })\n",
    "user_promo_data = pd.concat([user_promo_data, pd.DataFrame(negative_samples)], ignore_index=True)\n",
    "\n",
    "# Merge user features with promo usage data\n",
    "model_data = user_promo_data.merge(user_features, on='cif', how='left')\n",
    "\n",
    "# Prepare features and target\n",
    "X = model_data[['avg_transaction', 'total_spent', 'transaction_count', 'unique_merchants', 'unique_categories', 'account_age_days', 'promo_id']]\n",
    "y = model_data['used_promo']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LGBMClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "def recommend_promos(cif, top_n=5):\n",
    "    user_data = user_features[user_features['cif'] == cif].iloc[0]\n",
    "    promo_scores = []\n",
    "\n",
    "    for _, promo in promo_df.iterrows():\n",
    "        features = [user_data['avg_transaction'], user_data['total_spent'], \n",
    "                    user_data['transaction_count'], user_data['unique_merchants'], \n",
    "                    user_data['unique_categories'], user_data['account_age_days'], \n",
    "                    promo['promo_id']]\n",
    "        score = model.predict_proba([features])[0][1]  # Probability of class 1 (used_promo)\n",
    "        promo_scores.append((promo['promo_name'], score))\n",
    "\n",
    "    # Sort promos by score and return top N\n",
    "    recommended_promos = sorted(promo_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    return [promo for promo, _ in recommended_promos]\n",
    "\n",
    "# Example usage\n",
    "cif = user_features['cif'].iloc[0]  # Get the first CIF as an example\n",
    "recommended_promos = recommend_promos(cif)\n",
    "print(f\"\\nRecommended promos for user with CIF {cif}:\")\n",
    "for promo in recommended_promos:\n",
    "    print(promo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Connect to the database\n",
    "conn = psycopg2.connect(\n",
    "    dbname=\"postgres\",\n",
    "    user=\"postgres\",\n",
    "    password=\"\",\n",
    "    host=\"localhost\",\n",
    "    port=\"5432\"\n",
    ")\n",
    "\n",
    "# Load transaction data\n",
    "transaksi_df = pd.read_sql_query('''\n",
    "    SELECT id, cif, amount, \"date\", description, category, merchant_name, status_transaction, account_number\n",
    "    FROM public.transactions\n",
    "''', conn)\n",
    "\n",
    "# Load promo data\n",
    "promo_df = pd.read_sql_query('''\n",
    "    SELECT id, promo_name, category, merchant_name, startdate, enddate\n",
    "    FROM public.promo\n",
    "''', conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Preprocess the data\n",
    "def preprocess_data(transaksi_df, promo_df):\n",
    "    le_merchant = LabelEncoder()\n",
    "    le_category = LabelEncoder()\n",
    "    le_promo = LabelEncoder()\n",
    "\n",
    "    # Combine merchant names and categories from both dataframes\n",
    "    all_merchants = pd.concat([transaksi_df['merchant_name'], promo_df['merchant_name']]).unique()\n",
    "    all_categories = pd.concat([transaksi_df['category'], promo_df['category']]).unique()\n",
    "\n",
    "    # Fit label encoders on all unique values\n",
    "    le_merchant.fit(all_merchants)\n",
    "    le_category.fit(all_categories)\n",
    "    le_promo.fit(promo_df['promo_name'])\n",
    "\n",
    "    # Transform the data\n",
    "    transaksi_df['merchant_name'] = le_merchant.transform(transaksi_df['merchant_name'])\n",
    "    transaksi_df['category'] = le_category.transform(transaksi_df['category'])\n",
    "    promo_df['promo_id'] = le_promo.transform(promo_df['promo_name'])\n",
    "    promo_df['merchant_name'] = le_merchant.transform(promo_df['merchant_name'])\n",
    "    promo_df['category'] = le_category.transform(promo_df['category'])\n",
    "\n",
    "    # Convert date strings to datetime objects and sort\n",
    "    transaksi_df['date'] = pd.to_datetime(transaksi_df['date'])\n",
    "    promo_df['startdate'] = pd.to_datetime(promo_df['startdate'])\n",
    "    promo_df['enddate'] = pd.to_datetime(promo_df['enddate'])\n",
    "    transaksi_df = transaksi_df.sort_values('date')\n",
    "\n",
    "    return transaksi_df, promo_df, le_merchant, le_category, le_promo\n",
    "\n",
    "transaksi_df, promo_df, le_merchant, le_category, le_promo = preprocess_data(transaksi_df, promo_df)\n",
    "\n",
    "# Get the number of unique merchants and categories\n",
    "n_merchants = len(le_merchant.classes_)\n",
    "n_categories = len(le_category.classes_)\n",
    "\n",
    "print(f\"Number of unique merchants: {n_merchants}\")\n",
    "print(f\"Number of unique categories: {n_categories}\")\n",
    "print(f\"Merchant ID range: {transaksi_df['merchant_name'].min()} - {transaksi_df['merchant_name'].max()}\")\n",
    "print(f\"Category ID range: {transaksi_df['category'].min()} - {transaksi_df['category'].max()}\")\n",
    "\n",
    "def create_sequences(df, cif, sequence_length):\n",
    "    data = df[df['cif'] == cif].sort_values('date')\n",
    "    sequences = []\n",
    "    merchant_labels = []\n",
    "    category_labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        seq = data.iloc[i:i+sequence_length]\n",
    "        label = data.iloc[i+sequence_length]\n",
    "        sequences.append(seq[['amount', 'merchant_name', 'category']].values)\n",
    "        merchant_labels.append(label['merchant_name'])\n",
    "        category_labels.append(label['category'])\n",
    "    return np.array(sequences), np.array(merchant_labels), np.array(category_labels)\n",
    "\n",
    "# Create sequence data for all users\n",
    "sequence_length = 5  # You can adjust this\n",
    "all_sequences = []\n",
    "all_merchant_labels = []\n",
    "all_category_labels = []\n",
    "for cif in transaksi_df['cif'].unique():\n",
    "    sequences, merchant_labels, category_labels = create_sequences(transaksi_df, cif, sequence_length)\n",
    "    all_sequences.extend(sequences)\n",
    "    all_merchant_labels.extend(merchant_labels)\n",
    "    all_category_labels.extend(category_labels)\n",
    "\n",
    "all_sequences = np.array(all_sequences)\n",
    "all_merchant_labels = np.array(all_merchant_labels)\n",
    "all_category_labels = np.array(all_category_labels)\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = StandardScaler()\n",
    "all_sequences_flat = all_sequences.reshape(-1, all_sequences.shape[-1])\n",
    "all_sequences_normalized = scaler.fit_transform(all_sequences_flat).reshape(all_sequences.shape)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_merchant_train, y_merchant_test, y_category_train, y_category_test = train_test_split(\n",
    "    all_sequences_normalized, all_merchant_labels, all_category_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create model function\n",
    "def create_model(lstm_units=64, dropout_rate=0.2, learning_rate=0.001):\n",
    "    input_layer = Input(shape=(sequence_length, 3))\n",
    "    lstm1 = LSTM(lstm_units, return_sequences=True)(input_layer)\n",
    "    dropout1 = Dropout(dropout_rate)(lstm1)\n",
    "    lstm2 = LSTM(lstm_units)(dropout1)\n",
    "    dropout2 = Dropout(dropout_rate)(lstm2)\n",
    "    merchant_output = Dense(n_merchants, activation='softmax', name='merchant_output')(dropout2)\n",
    "    category_output = Dense(n_categories, activation='softmax', name='category_output')(dropout2)\n",
    "    \n",
    "    model = Model(inputs=input_layer, outputs=[merchant_output, category_output])\n",
    "    model.compile(optimizer=Adam(learning_rate), \n",
    "                  loss={'merchant_output': 'sparse_categorical_crossentropy', \n",
    "                        'category_output': 'sparse_categorical_crossentropy'},\n",
    "                  metrics={'merchant_output': 'accuracy', 'category_output': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# Train and evaluate model function\n",
    "def train_evaluate_model(model, X_train, y_merchant_train, y_category_train, X_test, y_merchant_test, y_category_test):\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    history = model.fit(X_train, {'merchant_output': y_merchant_train, 'category_output': y_category_train}, \n",
    "                        validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    y_merchant_pred, y_category_pred = model.predict(X_test)\n",
    "    y_merchant_pred_classes = np.argmax(y_merchant_pred, axis=1)\n",
    "    y_category_pred_classes = np.argmax(y_category_pred, axis=1)\n",
    "    \n",
    "    merchant_accuracy = accuracy_score(y_merchant_test, y_merchant_pred_classes)\n",
    "    category_accuracy = accuracy_score(y_category_test, y_category_pred_classes)\n",
    "    merchant_report = classification_report(y_merchant_test, y_merchant_pred_classes)\n",
    "    category_report = classification_report(y_category_test, y_category_pred_classes)\n",
    "    \n",
    "    return merchant_accuracy, category_accuracy, merchant_report, category_report, history\n",
    "\n",
    "# Hyperparameter tuning\n",
    "lstm_units_options = [32, 64, 128]\n",
    "dropout_rate_options = [0.2, 0.3, 0.4]\n",
    "learning_rate_options = [0.001, 0.01]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_params = {}\n",
    "best_model = None\n",
    "best_history = None\n",
    "\n",
    "for lstm_units in lstm_units_options:\n",
    "    for dropout_rate in dropout_rate_options:\n",
    "        for learning_rate in learning_rate_options:\n",
    "            model = create_model(lstm_units, dropout_rate, learning_rate)\n",
    "            merchant_accuracy, category_accuracy, merchant_report, category_report, history = train_evaluate_model(\n",
    "                model, X_train, y_merchant_train, y_category_train, X_test, y_merchant_test, y_category_test)\n",
    "            \n",
    "            avg_accuracy = (merchant_accuracy + category_accuracy) / 2\n",
    "            if avg_accuracy > best_accuracy:\n",
    "                best_accuracy = avg_accuracy\n",
    "                best_params = {'lstm_units': lstm_units, 'dropout_rate': dropout_rate, 'learning_rate': learning_rate}\n",
    "                best_model = model\n",
    "                best_history = history\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best average accuracy:\", best_accuracy)\n",
    "print(\"\\nMerchant Classification Report:\")\n",
    "print(merchant_report)\n",
    "print(\"\\nCategory Classification Report:\")\n",
    "print(category_report)\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(best_history.history['merchant_output_accuracy'], label='Merchant Training Accuracy')\n",
    "plt.plot(best_history.history['val_merchant_output_accuracy'], label='Merchant Validation Accuracy')\n",
    "plt.plot(best_history.history['category_output_accuracy'], label='Category Training Accuracy')\n",
    "plt.plot(best_history.history['val_category_output_accuracy'], label='Category Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(best_history.history['loss'], label='Total Loss')\n",
    "plt.plot(best_history.history['val_loss'], label='Total Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Recommend promos function\n",
    "def recommend_promos(cif, top_n=5):\n",
    "    user_data = transaksi_df[transaksi_df['cif'] == cif].sort_values('date').tail(sequence_length)\n",
    "    if len(user_data) < sequence_length:\n",
    "        return []  # Not enough data for this user\n",
    "    \n",
    "    user_sequence = user_data[['amount', 'merchant_name', 'category']].values\n",
    "    user_sequence_normalized = scaler.transform(user_sequence).reshape(1, sequence_length, 3)\n",
    "    \n",
    "    merchant_pred, category_pred = best_model.predict(user_sequence_normalized)\n",
    "    \n",
    "    top_merchants = le_merchant.inverse_transform(np.argsort(merchant_pred[0])[-top_n:])\n",
    "    top_categories = le_category.inverse_transform(np.argsort(category_pred[0])[-top_n:])\n",
    "    \n",
    "    matching_promos = promo_df[\n",
    "        (promo_df['merchant_name'].isin(top_merchants)) & \n",
    "        (promo_df['category'].isin(top_categories)) &\n",
    "        (promo_df['enddate'] >= datetime.now())\n",
    "    ]\n",
    "    \n",
    "    return matching_promos['promo_name'].tolist()[:top_n]\n",
    "\n",
    "# Example usage\n",
    "cif = transaksi_df['cif'].iloc[0]  # Get the first CIF as an example\n",
    "recommended_promos = recommend_promos(cif)\n",
    "print(f\"\\nRecommended promos for user with CIF {cif}:\")\n",
    "for promo in recommended_promos:\n",
    "    print(promo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
